"""
Script kiá»ƒm tra robot coordinates trong demo camera
"""
import cv2
import time
from detection.pipeline import ProcessingPipeline
from detection.camera import CameraInterface  
from detection.utils.tensorrt_yolo import YOLOTensorRT
from detection.utils.depth import DepthEstimator
import os

# Factory functions (copy tá»« use_tensorrt_example.py)
def create_camera():
    camera = CameraInterface(camera_index=0)
    camera.initialize()
    return camera

def create_yolo():
    ENGINE_PATH = "best.engine"  # Adjust path as needed
    return YOLOTensorRT(engine_path=ENGINE_PATH, conf=0.55)

def create_depth():
    return DepthEstimator(device='cpu', enable=False)  # Táº¯t depth Ä‘á»ƒ test nhanh

def test_robot_coordinates_in_pipeline():
    """Test robot coordinates tá»« pipeline."""
    print("ğŸ” TESTING ROBOT COORDINATES IN PIPELINE")
    print("="*60)
    
    # Táº¡o pipeline
    pipeline = ProcessingPipeline(
        camera_factory=create_camera,
        yolo_factory=create_yolo, 
        depth_factory=create_depth,
        max_queue_size=3
    )
    
    # Khá»Ÿi Ä‘á»™ng pipeline
    if not pipeline.start(timeout=60.0):
        print("âŒ KhÃ´ng thá»ƒ khá»Ÿi Ä‘á»™ng pipeline!")
        return
    
    print("âœ… Pipeline started successfully!")
    print("ğŸ“± Äá»£i phÃ¡t hiá»‡n objects Ä‘á»ƒ xem robot coordinates...")
    
    try:
        frame_count = 0
        while frame_count < 20:  # Test 20 frames
            # Láº¥y káº¿t quáº£ detection
            detection_result = pipeline.get_latest_detection()
            
            if detection_result:
                frame, detections = detection_result
                frame_count += 1
                
                print(f"\nğŸ“¸ Frame {frame_count}:")
                
                # â­ KIá»‚M TRA ROBOT COORDINATES â­
                robot_coords = detections.get('robot_coordinates', [])
                
                if robot_coords:
                    print(f"   âœ… CÃ“ {len(robot_coords)} robot coordinates:")
                    
                    for i, coord in enumerate(robot_coords, 1):
                        class_name = coord['class']
                        confidence = coord['confidence']
                        pixel = coord['camera_pixel']
                        robot_pos = coord['robot_coordinates']
                        cam_3d = coord.get('camera_3d')
                        
                        print(f"      {i}. {class_name} (conf: {confidence:.2f})")
                        print(f"         Pixel: ({pixel['x']}, {pixel['y']})")
                        print(f"         Robot: X={robot_pos['x']:8.2f}, Y={robot_pos['y']:8.2f}")
                        if cam_3d:
                            print(f"         Camera 3D: X={cam_3d['X']:8.3f}, Y={cam_3d['Y']:8.3f}, Z={cam_3d['Z']:8.3f}")
                else:
                    print("   âŒ KHÃ”NG cÃ³ robot coordinates")
                
                # Kiá»ƒm tra cÃ¡c data khÃ¡c
                print(f"   ğŸ“Š Other data:")
                print(f"      - Bounding boxes: {len(detections.get('bounding_boxes', []))}")
                print(f"      - Classes: {detections.get('classes', [])}")
                print(f"      - Theta4 result: {'CÃ“' if detections.get('theta4_result') else 'KHÃ”NG'}")
                print(f"      - Divided result: {'CÃ“' if detections.get('divided_result') else 'KHÃ”NG'}")
                
                # Hiá»ƒn thá»‹ frame
                cv2.imshow("Test Robot Coordinates", detections["annotated_frame"])
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("ğŸ‘‹ User requested exit")
                    break
            
            time.sleep(0.1)
    
    except KeyboardInterrupt:
        print("\nâš ï¸  Interrupted by user")
    
    finally:
        pipeline.stop()
        cv2.destroyAllWindows()
        print("\nâœ… Test completed!")

if __name__ == "__main__":
    test_robot_coordinates_in_pipeline() 