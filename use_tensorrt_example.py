"""
V√≠ d·ª• s·ª≠ d·ª•ng model TensorRT cho ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng
"""
import cv2
import time
import os
import threading
import numpy as np
from detection import (YOLOTensorRT,
                       ProcessingPipeline,
                       CameraInterface,
                       DepthEstimator,
                       ModuleDivision,
                       RegionManager)
from region_division_plc_integration import RegionDivisionPLCIntegration

# ƒê∆∞·ªùng d·∫´n t·ªõi file model - s·ª≠ d·ª•ng .pt thay v√¨ .engine ƒë·ªÉ tr√°nh l·ªói version
ENGINE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 
                          "utils", "detection", "best.engine")
# C·∫•u h√¨nh hi·ªÉn th·ªã depth - m·∫∑c ƒë·ªãnh l√† False ƒë·ªÉ tr√°nh lag
SHOW_DEPTH = os.environ.get('SHOW_DEPTH', 'false').lower() in ('true', '1', 'yes')
# C·∫•u h√¨nh hi·ªÉn th·ªã theta4 - m·∫∑c ƒë·ªãnh l√† False ƒë·ªÉ tr√°nh lag
SHOW_THETA4 = os.environ.get('SHOW_THETA4', 'true').lower() in ('true', '1', 'yes')
# C·∫•u h√¨nh hi·ªÉn th·ªã regions - m·∫∑c ƒë·ªãnh l√† True 
SHOW_REGIONS = os.environ.get('SHOW_REGIONS', 'true').lower() in ('true', '1', 'yes')

def draw_rotated_boxes_with_depth(image, detections, depth_results=None, thickness=2):
    """
    V·∫Ω rotated bounding boxes v·ªõi th√¥ng tin depth l√™n ·∫£nh.
    
    Args:
        image: ·∫¢nh ƒë·ªÉ v·∫Ω l√™n
        detections: K·∫øt qu·∫£ detection t·ª´ YOLO (ch·ª©a corners)
        depth_results: K·∫øt qu·∫£ depth estimation (optional)
        thickness: ƒê·ªô d√†y ƒë∆∞·ªùng vi·ªÅn
        
    Returns:
        np.ndarray: ·∫¢nh ƒë√£ ƒë∆∞·ª£c v·∫Ω boxes
    """
    result_image = image.copy()
    
    # M√†u s·∫Øc m·∫∑c ƒë·ªãnh cho c√°c boxes
    default_colors = [
        (0, 255, 0),    # Xanh l√°
        (255, 0, 0),    # ƒê·ªè
        (0, 0, 255),    # Xanh d∆∞∆°ng
        (255, 255, 0),  # V√†ng
        (255, 0, 255),  # T√≠m
        (0, 255, 255),  # Cyan
    ]
    
    # Ki·ªÉm tra xem c√≥ corners kh√¥ng
    if 'corners' in detections and detections['corners']:
        # S·ª≠ d·ª•ng rotated bounding boxes (corners)
        corners_list = detections['corners']
        
        for i, corners in enumerate(corners_list):
            # Ch·ªçn m√†u
            color = default_colors[i % len(default_colors)]
            
            # V·∫Ω rotated box b·∫±ng cv2.polylines
            pts = np.array(corners, np.int32)
            pts = pts.reshape((-1, 1, 2))
            cv2.polylines(result_image, [pts], True, color, thickness)
            
            # Th√™m th√¥ng tin depth n·∫øu c√≥
            if depth_results and i < len(depth_results):
                depth_info = depth_results[i]
                mean_depth = depth_info.get('mean_depth', 0.0)
                
                # T√¨m ƒëi·ªÉm tr√™n c√πng b√™n tr√°i ƒë·ªÉ ƒë·∫∑t text
                corners_array = np.array(corners)
                min_y_idx = np.argmin(corners_array[:, 1])
                text_x, text_y = corners_array[min_y_idx]
                text_y = max(text_y - 5, 10)  # ƒê·∫£m b·∫£o kh√¥ng v·∫Ω ra ngo√†i ·∫£nh
                
                # V·∫Ω text depth
                cv2.putText(result_image, f"{mean_depth:.1f}m", 
                           (int(text_x), int(text_y)), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                # V·∫Ω background cho text ƒë·ªÉ d·ªÖ ƒë·ªçc
                text_size = cv2.getTextSize(f"{mean_depth:.1f}m", cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                cv2.rectangle(result_image, 
                             (int(text_x) - 2, int(text_y) - text_size[1] - 2),
                             (int(text_x) + text_size[0] + 2, int(text_y) + 2),
                             (0, 0, 0), -1)
                cv2.putText(result_image, f"{mean_depth:.1f}m", 
                           (int(text_x), int(text_y)), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    
    elif 'bounding_boxes' in detections and detections['bounding_boxes']:
        # Fallback: s·ª≠ d·ª•ng regular bounding boxes n·∫øu kh√¥ng c√≥ corners
        print("[WARNING] Kh√¥ng c√≥ corners, s·ª≠ d·ª•ng regular bounding boxes")
        bboxes = detections['bounding_boxes']
        
        for i, bbox in enumerate(bboxes):
            color = default_colors[i % len(default_colors)]
            x1, y1, x2, y2 = [int(v) for v in bbox]
            
            # V·∫Ω regular box
            cv2.rectangle(result_image, (x1, y1), (x2, y2), color, thickness)
            
            # Th√™m th√¥ng tin depth n·∫øu c√≥
            if depth_results and i < len(depth_results):
                depth_info = depth_results[i]
                mean_depth = depth_info.get('mean_depth', 0.0)
                cv2.putText(result_image, f"{mean_depth:.1f}m", (x1, y1 - 5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    
    return result_image

def draw_depth_regions_with_rotated_boxes(image, depth_results):
    """
    V·∫Ω depth regions v·ªõi rotated bounding boxes (cho pipeline camera).
    Ph√¢n bi·ªát pallet regions v√† non-pallet objects.
    
    Args:
        image: ·∫¢nh ƒë·ªÉ v·∫Ω l√™n
        depth_results: K·∫øt qu·∫£ depth t·ª´ pipeline
        
    Returns:
        np.ndarray: ·∫¢nh ƒë√£ ƒë∆∞·ª£c v·∫Ω
    """
    result_image = image.copy()
    
    # M√†u s·∫Øc cho c√°c region
    pallet_colors = [(0, 255, 0), (255, 0, 0), (0, 0, 255)]  # Xanh, ƒë·ªè, xanh d∆∞∆°ng cho pallet regions
    non_pallet_color = (255, 255, 0)  # V√†ng cho non-pallet objects
    
    for i, region_data in enumerate(depth_results):
        # L·∫•y th√¥ng tin t·ª´ depth result
        region_info = region_data.get('region_info', {})
        position = region_data.get('position', {})
        
        # Ph√¢n bi·ªát pallet v√† non-pallet
        pallet_id = region_info.get('pallet_id', 0)
        is_pallet = pallet_id > 0
        
        if is_pallet:
            # Pallet: Ch·ªçn m√†u d·ª±a tr√™n region_id
            region_id = region_info.get('region_id', 1)
            color = pallet_colors[(region_id - 1) % len(pallet_colors)]
            thickness = 2
        else:
            # Non-pallet: S·ª≠ d·ª•ng m√†u v√†ng, ƒë∆∞·ªùng vi·ªÅn d√†y h∆°n
            color = non_pallet_color
            thickness = 3
        
        # ∆Øu ti√™n s·ª≠ d·ª•ng corners n·∫øu c√≥ (t·ª´ rotated boxes)
        if 'corners' in region_data and region_data['corners']:
            corners = region_data['corners']
            pts = np.array(corners, np.int32)
            pts = pts.reshape((-1, 1, 2))
            cv2.polylines(result_image, [pts], True, color, thickness)
            
            # T√¨m ƒëi·ªÉm ƒë·ªÉ ƒë·∫∑t text - s·ª≠ d·ª•ng ƒëi·ªÉm c√≥ y nh·ªè nh·∫•t
            corners_array = np.array(corners)
            min_y_idx = np.argmin(corners_array[:, 1])
            text_x, text_y = corners_array[min_y_idx]
            text_y = max(text_y - 5, 10)
            
        else:
            # Fallback: s·ª≠ d·ª•ng bbox n·∫øu kh√¥ng c√≥ corners
            bbox = region_data.get('bbox', [])
            if len(bbox) >= 4:
                x1, y1, x2, y2 = [int(v) for v in bbox]
                cv2.rectangle(result_image, (x1, y1), (x2, y2), color, thickness)
                text_x, text_y = x1, y1 - 5
            else:
                print(f"[WARNING] Region kh√¥ng c√≥ corners ho·∫∑c bbox h·ª£p l·ªá")
                continue
        
        # Hi·ªÉn th·ªã th√¥ng tin region v√† depth
        depth_z = position.get('z', 0.0)
        
        if is_pallet:
            # Pallet: hi·ªÉn th·ªã th√¥ng tin region
            region_id = region_info.get('region_id', 1)
            text = f"P{pallet_id}R{region_id}: {depth_z:.1f}m"
        else:
            # Non-pallet: hi·ªÉn th·ªã class
            object_class = region_info.get('object_class', 'Unknown')
            text = f"C{object_class}: {depth_z:.1f}m"
        
        # V·∫Ω background cho text ƒë·ªÉ d·ªÖ ƒë·ªçc
        text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
        cv2.rectangle(result_image, 
                     (int(text_x) - 2, int(text_y) - text_size[1] - 2),
                     (int(text_x) + text_size[0] + 2, int(text_y) + 2),
                     (0, 0, 0), -1)
        
        cv2.putText(result_image, text, (int(text_x), int(text_y)), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    return result_image

def draw_theta4_visualization(image, detections_with_theta4):
    """
    V·∫Ω theta4 visualization v·ªõi regions, loads v√† rotation angles t·ª´ pipeline.
    
    Args:
        image: ·∫¢nh g·ªëc
        detections_with_theta4: K·∫øt qu·∫£ detection t·ª´ pipeline bao g·ªìm theta4_result
        
    Returns:
        np.ndarray: ·∫¢nh ƒë√£ ƒë∆∞·ª£c v·∫Ω v·ªõi theta4 info
    """
    result_image = image.copy()
    
    # L·∫•y theta4 result t·ª´ detection
    theta4_result = detections_with_theta4.get('theta4_result')
    if theta4_result is None:
        # N·∫øu kh√¥ng c√≥ theta4 result, ch·ªâ hi·ªÉn th·ªã th√¥ng b√°o
        cv2.putText(result_image, "No Theta4 calculation available", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
        return result_image
    
    # M√†u s·∫Øc
    pallet_color = (255, 0, 0)      # ƒê·ªè cho pallet
    load_color = (0, 255, 0)        # Xanh l√° cho load
    region_color = (0, 255, 255)    # V√†ng cho region boundaries
    mapping_color = (255, 0, 255)   # Magenta cho mapping lines
    
    # V·∫Ω regions t·ª´ module division
    regions_data = theta4_result['regions_result']['regions_data']
    for i, region in enumerate(regions_data):
        draw_region_boundary_theta4(result_image, region, region_color, i+1)
    
    # V·∫Ω load objects v√† theta4 mappings
    theta4_results = theta4_result['theta4_results']
    for i, theta4_calc in enumerate(theta4_results):
        mapping = theta4_calc['mapping']
        load_obj = mapping['load_object']
        
        # V·∫Ω load v·ªõi orientation
        draw_object_with_orientation_theta4(result_image, load_obj, load_color, "LOAD")
        
        # V·∫Ω ƒë∆∞·ªùng mapping v√† theta4 command n·∫øu th√†nh c√¥ng
        if mapping['mapping_success'] and theta4_calc['theta4_success']:
            target_region = mapping['target_region']
            load_center = load_obj['center']
            region_center = target_region['center']
            
            # V·∫Ω ƒë∆∞·ªùng mapping
            cv2.line(result_image, 
                    (int(load_center[0]), int(load_center[1])),
                    (int(region_center[0]), int(region_center[1])),
                    mapping_color, 2)
            
            # V·∫Ω theta4 command g·∫ßn load
            theta4_command = theta4_calc.get('theta4_command', 'NO THETA4')
            rotation_direction = theta4_calc.get('rotation_direction', '')
            
            # Text theta4 command
            draw_text_with_background_theta4(result_image, theta4_command,
                                           (int(load_center[0]) + 15, int(load_center[1]) + 35),
                                           (255, 255, 255), (0, 0, 0))
            
            # Text h∆∞·ªõng xoay (ng·∫Øn g·ªçn)
            direction_short = "CW" if "Clockwise" in rotation_direction else ("CCW" if "Counter" in rotation_direction else "OK")
            draw_text_with_background_theta4(result_image, f"({direction_short})",
                                           (int(load_center[0]) + 15, int(load_center[1]) + 55),
                                           (255, 255, 0), (0, 0, 0))
    
    # V·∫Ω pallet objects
    for pallet in theta4_result['pallet_objects']:
        draw_object_with_orientation_theta4(result_image, pallet, pallet_color, "PALLET")
    
    # V·∫Ω summary info ·ªü g√≥c tr√™n
    draw_summary_info_theta4(result_image, theta4_result)
    
    # V·∫Ω coordinate compass
    draw_coordinate_compass_theta4(result_image)
    
    return result_image

def draw_object_with_orientation_theta4(image, obj, color, obj_type):
    """V·∫Ω object v·ªõi orientation arrow cho theta4 visualization"""
    import math
    
    cx, cy = obj['center']
    x_axis_angle = obj['x_axis_angle']
    length = obj['length']
    width = obj['width']
    
    # V·∫Ω center point
    cv2.circle(image, (int(cx), int(cy)), 4, color, -1)
    
    # V·∫Ω orientation arrow (theo h·ªá t·ªça ƒë·ªô custom)
    arrow_length = 30
    end_x = cx - arrow_length * math.cos(math.radians(x_axis_angle))
    end_y = cy - arrow_length * math.sin(math.radians(x_axis_angle))
    cv2.arrowedLine(image, (int(cx), int(cy)), 
                   (int(end_x), int(end_y)), color, 2)
    
    # V·∫Ω th√¥ng tin object
    text1 = f"{obj_type}: {length:.0f}x{width:.0f}"
    text2 = f"{x_axis_angle:.1f}¬∞"
    draw_text_with_background_theta4(image, text1, 
                                   (int(cx) + 8, int(cy) - 25), color, (0, 0, 0))
    draw_text_with_background_theta4(image, text2, 
                                   (int(cx) + 8, int(cy) - 10), color, (0, 0, 0))

def draw_region_boundary_theta4(image, region, color, region_num):
    """V·∫Ω boundary c·ªßa region cho theta4 visualization"""
    if 'corners' in region:
        # V·∫Ω OBB corners
        corners = region['corners']
        corners_int = [(int(x), int(y)) for x, y in corners]
        cv2.polylines(image, [np.array(corners_int)], True, color, 2)
    else:
        # V·∫Ω regular bounding box
        bbox = region['bbox']
        x1, y1, x2, y2 = [int(x) for x in bbox]
        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
    
    # V·∫Ω region number
    center = region['center']
    draw_text_with_background_theta4(image, f"R{region_num}", 
                                   (int(center[0]) - 10, int(center[1])), 
                                   color, (0, 0, 0))

def draw_summary_info_theta4(image, theta4_result):
    """V·∫Ω th√¥ng tin summary cho theta4 visualization"""
    summary = theta4_result['summary']
    layer = theta4_result['layer']
    
    y_offset = 20
    line_height = 18
    
    info_lines = [
        f"Theta4 Layer: {layer}",
        f"P:{summary['total_pallets']} L:{summary['total_loads']} R:{summary['total_regions']}",
        f"Map:{summary['successful_mappings']} Œ∏4:{summary['successful_theta4']}",
        f"Time:{theta4_result['processing_time']*1000:.0f}ms"
    ]
    
    for i, line in enumerate(info_lines):
        draw_text_with_background_theta4(image, line,
                                       (10, y_offset + i * line_height),
                                       (255, 255, 255), (0, 0, 0))

def draw_text_with_background_theta4(image, text, position, color, bg_color=(0, 0, 0)):
    """V·∫Ω text v·ªõi background cho theta4 visualization"""
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.4
    thickness = 1
    
    text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]
    
    cv2.rectangle(image, 
                 (position[0] - 2, position[1] - text_size[1] - 2),
                 (position[0] + text_size[0] + 2, position[1] + 2),
                 bg_color, -1)
    
    cv2.putText(image, text, position, font, font_scale, color, thickness)

def draw_coordinate_compass_theta4(image):
    """V·∫Ω compass h·ªá t·ªça ƒë·ªô custom cho theta4 visualization"""
    h, w = image.shape[:2]
    center_x = w - 60
    center_y = h - 60
    
    # Background
    cv2.rectangle(image, (center_x - 35, center_y - 35), 
                 (center_x + 35, center_y + 35), (0, 0, 0), -1)
    cv2.rectangle(image, (center_x - 35, center_y - 35), 
                 (center_x + 35, center_y + 35), (255, 255, 255), 1)
    
    # Tr·ª•c X+ (ƒë·ªè, E‚ÜíW)
    cv2.arrowedLine(image, (center_x, center_y), 
                   (center_x - 25, center_y), (0, 0, 255), 2)
    cv2.putText(image, "X+", (center_x - 35, center_y + 3), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 1)
    
    # Tr·ª•c Y+ (xanh l√°, N‚ÜíS)
    cv2.arrowedLine(image, (center_x, center_y), 
                   (center_x, center_y + 25), (0, 255, 0), 2)
    cv2.putText(image, "Y+", (center_x + 3, center_y + 35), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1)

def draw_region_visualization(image, detections_with_regions):
    """
    V·∫Ω regions v√† detections theo regions t·ª´ pipeline.
    
    Args:
        image: ·∫¢nh g·ªëc
        detections_with_regions: K·∫øt qu·∫£ detection t·ª´ pipeline bao g·ªìm region_filtered
        
    Returns:
        np.ndarray: ·∫¢nh ƒë√£ ƒë∆∞·ª£c v·∫Ω v·ªõi regions v√† detections
    """
    result_image = image.copy()
    
    # L·∫•y region_filtered t·ª´ detections
    region_filtered = detections_with_regions.get('region_filtered')
    if not region_filtered:
        return result_image
    
    # Kh·ªüi t·∫°o RegionManager ƒë·ªÉ v·∫Ω regions (ch·ªâ ƒë·ªÉ v·∫Ω, kh√¥ng x·ª≠ l√Ω)
    temp_region_manager = RegionManager()
    
    # V·∫Ω t·∫•t c·∫£ regions tr∆∞·ªõc
    result_image = temp_region_manager.draw_regions(result_image, show_labels=True)
    
    # V·∫Ω detections theo t·ª´ng region
    for region_name, region_data in region_filtered['regions'].items():
        if not region_data['bounding_boxes']:
            continue
        
        region_info = region_data['region_info']
        color = region_info['color']
        bboxes = region_data['bounding_boxes']
        classes = region_data['classes']
        corners_list = region_data.get('corners', [])
        
        # V·∫Ω detections trong region n√†y
        for i, bbox in enumerate(bboxes):
            # ∆Øu ti√™n v·∫Ω corners n·∫øu c√≥
            if i < len(corners_list) and corners_list[i]:
                corners = corners_list[i]
                pts = np.array(corners, np.int32)
                pts = pts.reshape((-1, 1, 2))
                cv2.polylines(result_image, [pts], True, color, 3)
                
                # V·∫Ω ƒëi·ªÉm center
                center_x = int(np.mean([p[0] for p in corners]))
                center_y = int(np.mean([p[1] for p in corners]))
            else:
                # Fallback: v·∫Ω regular bbox
                x1, y1, x2, y2 = [int(v) for v in bbox]
                cv2.rectangle(result_image, (x1, y1), (x2, y2), color, 3)
                center_x = (x1 + x2) // 2
                center_y = (y1 + y2) // 2
            
            # V·∫Ω center point
            cv2.circle(result_image, (center_x, center_y), 5, color, -1)
            
            # V·∫Ω th√¥ng tin class
            class_id = classes[i] if i < len(classes) else 0
            class_names = {0: 'L', 1: 'L2', 2: 'P'}  # T√™n ng·∫Øn
            class_name = class_names.get(class_id, str(int(class_id)))
            
            # V·∫Ω text v·ªõi background
            text = f"{region_name}:{class_name}"
            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
            cv2.rectangle(result_image,
                        (center_x - text_size[0]//2 - 2, center_y - 25 - text_size[1] - 2),
                        (center_x + text_size[0]//2 + 2, center_y - 25 + 2),
                        (0, 0, 0), -1)
            cv2.putText(result_image, text,
                      (center_x - text_size[0]//2, center_y - 25),
                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    # V·∫Ω unassigned detections n·∫øu c√≥
    unassigned_data = region_filtered.get('unassigned', {})
    if unassigned_data.get('bounding_boxes'):
        unassigned_color = (128, 128, 128)  # X√°m cho unassigned
        bboxes = unassigned_data['bounding_boxes']
        classes = unassigned_data.get('classes', [])
        corners_list = unassigned_data.get('corners', [])
        
        for i, bbox in enumerate(bboxes):
            # ∆Øu ti√™n v·∫Ω corners n·∫øu c√≥
            if i < len(corners_list) and corners_list[i]:
                corners = corners_list[i]
                pts = np.array(corners, np.int32)
                pts = pts.reshape((-1, 1, 2))
                cv2.polylines(result_image, [pts], True, unassigned_color, 2)
                
                center_x = int(np.mean([p[0] for p in corners]))
                center_y = int(np.mean([p[1] for p in corners]))
            else:
                # Fallback: v·∫Ω regular bbox
                x1, y1, x2, y2 = [int(v) for v in bbox]
                cv2.rectangle(result_image, (x1, y1), (x2, y2), unassigned_color, 2)
                center_x = (x1 + x2) // 2
                center_y = (y1 + y2) // 2
            
            # V·∫Ω center point
            cv2.circle(result_image, (center_x, center_y), 3, unassigned_color, -1)
            
            # V·∫Ω th√¥ng tin class
            class_id = classes[i] if i < len(classes) else 0
            class_names = {0: 'L', 1: 'L2', 2: 'P'}
            class_name = class_names.get(class_id, str(int(class_id)))
            
            text = f"UNASSIGNED:{class_name}"
            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)[0]
            cv2.rectangle(result_image,
                        (center_x - text_size[0]//2 - 2, center_y - 20 - text_size[1] - 2),
                        (center_x + text_size[0]//2 + 2, center_y - 20 + 2),
                        (0, 0, 0), -1)
            cv2.putText(result_image, text,
                      (center_x - text_size[0]//2, center_y - 20),
                      cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
    
    # V·∫Ω th·ªëng k√™ ·ªü g√≥c tr√™n b√™n tr√°i
    y_offset = 10
    line_height = 20
    
    region_counts = {}
    for region_name, region_data in region_filtered['regions'].items():
        count = len(region_data['bounding_boxes'])
        if count > 0:
            region_counts[region_name] = count
    
    unassigned_count = len(region_filtered['unassigned']['bounding_boxes'])
    
    stats_lines = [
        "=== REGION STATISTICS ===",
        f"Total Detections: {len(detections_with_regions.get('bounding_boxes', []))}",
    ]
    
    for region_name, count in region_counts.items():
        stats_lines.append(f"{region_name}: {count}")
    
    if unassigned_count > 0:
        stats_lines.append(f"Unassigned: {unassigned_count}")
    
    for i, line in enumerate(stats_lines):
        # V·∫Ω background cho text
        text_size = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
        cv2.rectangle(result_image,
                    (10 - 2, y_offset + i * line_height - text_size[1] - 2),
                    (10 + text_size[0] + 2, y_offset + i * line_height + 2),
                    (0, 0, 0), -1)
        
        # Ch·ªçn m√†u text
        if i == 0:  # Header
            color = (255, 255, 0)  # V√†ng
        elif "Unassigned" in line:
            color = (128, 128, 128)  # X√°m
        else:
            color = (255, 255, 255)  # Tr·∫Øng
        
        cv2.putText(result_image, line,
                  (10, y_offset + i * line_height),
                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
    
    return result_image

def demo_single_image():
    """Th·ª≠ nghi·ªám v·ªõi m·ªôt ·∫£nh ƒë∆°n l·∫ª"""
    print("Th·ª≠ nghi·ªám ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng v·ªõi TensorRT tr√™n m·ªôt ·∫£nh ƒë∆°n l·∫ª")
    
    # Kh·ªüi t·∫°o model YOLO
    model = YOLOTensorRT(engine_path=ENGINE_PATH, conf=0.25)
    
    # Kh·ªüi t·∫°o model Depth (s·ª≠ d·ª•ng chung config v·ªõi camera)
    depth_model = create_depth()
    print(f"Depth model enable: {depth_model.enable}")
    
    # Kh·ªüi t·∫°o Module Division
    divider = ModuleDivision()
    print("Module Division ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o")
    
    # Hi·ªÉn th·ªã ·∫£nh c√≥ s·∫µn t·ª´ folder images_pallets2
    print("\n·∫¢nh c√≥ s·∫µn trong folder images_pallets2:")
    pallets_folder = "images_pallets2"
    if os.path.exists(pallets_folder):
        image_files = [f for f in os.listdir(pallets_folder) if f.endswith(('.jpg', '.jpeg', '.png'))]
        image_files.sort()  # S·∫Øp x·∫øp theo th·ª© t·ª±
        for i, img_file in enumerate(image_files, 1):
            print(f"  {i}. {img_file}")
        print(f"  0. Nh·∫≠p ƒë∆∞·ªùng d·∫´n kh√°c")
        
        choice = input(f"\nCh·ªçn ·∫£nh (1-{len(image_files)}) ho·∫∑c 0 ƒë·ªÉ nh·∫≠p ƒë∆∞·ªùng d·∫´n kh√°c: ")
        
        try:
            choice_num = int(choice)
            if 1 <= choice_num <= len(image_files):
                image_path = os.path.join(pallets_folder, image_files[choice_num - 1])
                print(f"ƒê√£ ch·ªçn: {image_path}")
            elif choice_num == 0:
                image_path = input("Nh·∫≠p ƒë∆∞·ªùng d·∫´n t·ªõi ·∫£nh th·ª≠ nghi·ªám: ")
                if not image_path:
                    image_path = "test.jpg"  # ·∫¢nh m·∫∑c ƒë·ªãnh
            else:
                print("L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng ·∫£nh ƒë·∫ßu ti√™n")
                image_path = os.path.join(pallets_folder, image_files[0])
        except ValueError:
            print("L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng ·∫£nh ƒë·∫ßu ti√™n")
            image_path = os.path.join(pallets_folder, image_files[0])
    else:
        # ƒê·ªçc ·∫£nh th·ª≠ nghi·ªám theo c√°ch c≈© n·∫øu kh√¥ng t√¨m th·∫•y folder
        image_path = input("Nh·∫≠p ƒë∆∞·ªùng d·∫´n t·ªõi ·∫£nh th·ª≠ nghi·ªám: ")
        if not image_path:
            image_path = "test.jpg"  # ·∫¢nh m·∫∑c ƒë·ªãnh
    
    frame = cv2.imread(image_path)
    if frame is None:
        print(f"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh t·ª´ {image_path}")
        return
    
    # Hi·ªÉn th·ªã th√¥ng tin ·∫£nh
    height, width = frame.shape[:2]
    print(f"\nTh√¥ng tin ·∫£nh:")
    print(f"  ƒê∆∞·ªùng d·∫´n: {image_path}")
    print(f"  K√≠ch th∆∞·ªõc: {width}x{height}")
    print(f"  K√≠ch th∆∞·ªõc file: {os.path.getsize(image_path)} bytes")
    
    # ƒêo th·ªùi gian x·ª≠ l√Ω YOLO
    start_time = time.time()
    
    # Th·ª±c hi·ªán ph√°t hi·ªán YOLO
    detections = model.detect(frame)
    
    yolo_time = time.time()
    
    # Chia pallet th√†nh c√°c v√πng nh·ªè v√† th·ª±c hi·ªán depth estimation
    depth_results = None
    region_depth_results = []
    if depth_model.enable and len(detections['bounding_boxes']) > 0:
        print("ƒêang chia pallet th√†nh c√°c v√πng nh·ªè...")
        
        # Chia pallet th√†nh c√°c v√πng s·ª≠ d·ª•ng Module Division
        divided_result = divider.process_pallet_detections(detections, layer=1)
        depth_regions = divider.prepare_for_depth_estimation(divided_result)
        
        print(f"ƒê√£ chia th√†nh {len(depth_regions)} v√πng")
        
        print("ƒêang x·ª≠ l√Ω depth estimation cho t·ª´ng v√πng...")
        # Th·ª±c hi·ªán depth estimation cho t·ª´ng v√πng
        for i, region in enumerate(depth_regions):
            bbox = region['bbox']
            region_info = region['region_info']
            
            # ∆Ø·ªõc t√≠nh ƒë·ªô s√¢u cho bbox n√†y
            region_depth = depth_model.estimate_depth(frame, [bbox])
            
            # T·∫°o k·∫øt qu·∫£ chi ti·∫øt cho region
            if region_depth and len(region_depth) > 0:
                depth_info = region_depth[0]  # L·∫•y k·∫øt qu·∫£ ƒë·∫ßu ti√™n
                result = {
                    'region_info': region_info,
                    'bbox': bbox,
                    'center': region['center'],
                    'depth': depth_info,
                    'position': {
                        'x': region['center'][0],
                        'y': region['center'][1], 
                        'z': depth_info.get('mean_depth', 0.0) if isinstance(depth_info, dict) else 0.0
                    }
                }
                
                # Th√™m corners n·∫øu c√≥ (ƒë·ªÉ v·∫Ω rotated boxes)
                if 'corners' in region:
                    result['corners'] = region['corners']
                
                # Th√™m corners g·ªëc c·ªßa pallet n·∫øu c√≥
                if 'original_corners' in region:
                    result['original_corners'] = region['original_corners']
                
                region_depth_results.append(result)
        
        # Gi·ªØ l·∫°i depth_results c≈© ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code hi·ªÉn th·ªã
        if region_depth_results:
            depth_results = [r['depth'] for r in region_depth_results]
        
    depth_time = time.time()
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    print(f"Th·ªùi gian x·ª≠ l√Ω YOLO: {(yolo_time - start_time) * 1000:.2f} ms")
    if depth_model.enable:
        print(f"Th·ªùi gian x·ª≠ l√Ω Depth: {(depth_time - yolo_time) * 1000:.2f} ms")
        print(f"T·ªïng th·ªùi gian: {(depth_time - start_time) * 1000:.2f} ms")
    print(f"ƒê√£ ph√°t hi·ªán {len(detections['bounding_boxes'])} ƒë·ªëi t∆∞·ª£ng")
    
    # Hi·ªÉn th·ªã th√¥ng tin depth n·∫øu c√≥
    if region_depth_results and len(region_depth_results) > 0:
        print("Th√¥ng tin ƒë·ªô s√¢u theo v√πng:")
        for i, result in enumerate(region_depth_results):
            region_info = result['region_info']
            depth_info = result['depth']
            position = result['position']
            
            pallet_id = region_info.get('pallet_id', 1)
            region_id = region_info.get('region_id', 1)
            layer = region_info.get('layer', 1)
            
            if isinstance(depth_info, dict):
                mean_depth = depth_info.get('mean_depth', 0.0)
                min_depth = depth_info.get('min_depth', 0.0) 
                max_depth = depth_info.get('max_depth', 0.0)
            else:
                mean_depth = min_depth = max_depth = 0.0
            
            print(f"  Pallet {pallet_id}, V√πng {region_id} (Layer {layer}): {mean_depth:.2f}m (min: {min_depth:.2f}m, max: {max_depth:.2f}m)")
            print(f"    T·ªça ƒë·ªô pixel: X={position['x']:.1f}, Y={position['y']:.1f}, Z={position['z']:.2f}m")
            
            # Hi·ªÉn th·ªã th√¥ng tin 3D n·∫øu c√≥ camera calibration
            if 'position_3d_camera' in result:
                pos_3d = result['position_3d_camera']
                print(f"    T·ªça ƒë·ªô 3D (camera): X={pos_3d['X']:.3f}m, Y={pos_3d['Y']:.3f}m, Z={pos_3d['Z']:.3f}m")
            
            if 'real_size' in result:
                real_size = result['real_size']
                print(f"    K√≠ch th∆∞·ªõc th·ª±c: {real_size['width_m']:.3f}m x {real_size['height_m']:.3f}m (di·ªán t√≠ch: {real_size['area_m2']:.3f}m¬≤)")
    elif depth_results and len(depth_results) > 0:
        # Fallback cho tr∆∞·ªùng h·ª£p kh√¥ng c√≥ region results
        print("Th√¥ng tin ƒë·ªô s√¢u (kh√¥ng chia v√πng):")
        for i, result in enumerate(depth_results):
            if isinstance(result, dict):
                print(f"  ƒê·ªëi t∆∞·ª£ng {i+1}: {result.get('mean_depth', 0.0):.2f}m (min: {result.get('min_depth', 0.0):.2f}m, max: {result.get('max_depth', 0.0):.2f}m)")
            else:
                print(f"  ƒê·ªëi t∆∞·ª£ng {i+1}: Kh√¥ng c√≥ th√¥ng tin depth")
    
    # Hi·ªÉn th·ªã ·∫£nh detection t·ª´ YOLO
    cv2.imshow("K·∫øt qu·∫£ ph√°t hi·ªán", detections["annotated_frame"])
    
    # Hi·ªÉn th·ªã rotated boxes v·ªõi depth information
    if detections['corners'] or detections['bounding_boxes']:
        depth_viz = draw_rotated_boxes_with_depth(frame, detections, depth_results)
        cv2.imshow("Rotated Boxes v·ªõi Depth Information", depth_viz)
        
        # Hi·ªÉn th·ªã depth regions n·∫øu c√≥ chia v√πng
        if region_depth_results:
            region_viz = draw_depth_regions_with_rotated_boxes(frame, region_depth_results)
            cv2.imshow("Depth Regions (Module Division)", region_viz)
    
    print("\n·∫¢nh ƒë√£ ƒë∆∞·ª£c hi·ªÉn th·ªã, vui l√≤ng nh·∫•n ph√≠m b·∫•t k·ª≥ ƒë·ªÉ ti·∫øp t·ª•c")
    cv2.waitKey(0)
    # L∆∞u k·∫øt qu·∫£
    save_choice = input("\nB·∫°n c√≥ mu·ªën l∆∞u k·∫øt qu·∫£? (y/n): ").lower()
    if save_choice in ['y', 'yes']:
        # T·∫°o t√™n file output
        base_name = os.path.splitext(os.path.basename(image_path))[0]
        detection_output_path = f"result_{base_name}.jpg"
        cv2.imwrite(detection_output_path, detections["annotated_frame"])
        print(f"ƒê√£ l∆∞u k·∫øt qu·∫£ detection t·∫°i: {detection_output_path}")
        
        # L∆∞u rotated boxes v·ªõi depth n·∫øu c√≥
        if detections['corners'] or detections['bounding_boxes']:
            depth_viz = draw_rotated_boxes_with_depth(frame, detections, depth_results)
            depth_output_path = f"rotated_depth_{base_name}.jpg"
            cv2.imwrite(depth_output_path, depth_viz)
            print(f"ƒê√£ l∆∞u k·∫øt qu·∫£ rotated boxes v·ªõi depth t·∫°i: {depth_output_path}")
            
            # L∆∞u depth regions n·∫øu c√≥ chia v√πng
            if region_depth_results:
                region_viz = draw_depth_regions_with_rotated_boxes(frame, region_depth_results)
                region_output_path = f"depth_regions_{base_name}.jpg"
                cv2.imwrite(region_output_path, region_viz)
                print(f"ƒê√£ l∆∞u k·∫øt qu·∫£ depth regions t·∫°i: {region_output_path}")
    
    cv2.destroyAllWindows()

def demo_batch_images():
    """Th·ª≠ nghi·ªám v·ªõi t·∫•t c·∫£ ·∫£nh trong folder images_pallets2"""
    print("Th·ª≠ nghi·ªám ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng v·ªõi TensorRT tr√™n t·∫•t c·∫£ ·∫£nh trong folder")
    
    pallets_folder = "images_pallets"
    if not os.path.exists(pallets_folder):
        print(f"Kh√¥ng t√¨m th·∫•y folder {pallets_folder}")
        return
    
    # Kh·ªüi t·∫°o model YOLO
    model = YOLOTensorRT(engine_path=ENGINE_PATH, conf=0.25)
    
    # Kh·ªüi t·∫°o model Depth (s·ª≠ d·ª•ng chung config v·ªõi camera)
    depth_model = create_depth()
    print(f"Depth model enable: {depth_model.enable}")
    
    # Kh·ªüi t·∫°o Module Division
    divider = ModuleDivision()
    print("Module Division ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o")
    
    # L·∫•y danh s√°ch ·∫£nh
    image_files = [f for f in os.listdir(pallets_folder) if f.endswith(('.jpg', '.jpeg', '.png'))]
    image_files.sort()
    
    if not image_files:
        print("Kh√¥ng c√≥ ·∫£nh n√†o trong folder")
        return
    
    print(f"T√¨m th·∫•y {len(image_files)} ·∫£nh")
    
    # T·∫°o folder k·∫øt qu·∫£
    output_folder = "batch_results"
    os.makedirs(output_folder, exist_ok=True)
    
    # T·∫°o subfolder cho rotated boxes v·ªõi depth
    rotated_folder = os.path.join(output_folder, "rotated_depth")
    os.makedirs(rotated_folder, exist_ok=True)
    
    # T·∫°o subfolder cho depth regions 
    regions_folder = os.path.join(output_folder, "depth_regions")
    os.makedirs(regions_folder, exist_ok=True)
    
    total_time = 0
    total_yolo_time = 0
    total_depth_time = 0
    successful_detections = 0
    successful_depth_detections = 0
    
    for i, img_file in enumerate(image_files, 1):
        image_path = os.path.join(pallets_folder, img_file)
        print(f"\n[{i}/{len(image_files)}] X·ª≠ l√Ω: {img_file}")
        
        frame = cv2.imread(image_path)
        if frame is None:
            print(f"  Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {img_file}")
            continue
        
        # ƒêo th·ªùi gian x·ª≠ l√Ω YOLO
        start_time = time.time()
        
        # Th·ª±c hi·ªán ph√°t hi·ªán YOLO
        detections = model.detect(frame)
        
        yolo_time = time.time()
        yolo_process_time = (yolo_time - start_time) * 1000
        total_yolo_time += yolo_process_time
        
        # Chia pallet th√†nh c√°c v√πng nh·ªè v√† th·ª±c hi·ªán depth estimation
        depth_results = None
        region_depth_results = []
        depth_process_time = 0
        if depth_model.enable and len(detections['bounding_boxes']) > 0:
            # Chia pallet th√†nh c√°c v√πng s·ª≠ d·ª•ng Module Division
            divided_result = divider.process_pallet_detections(detections, layer=2)
            depth_regions = divider.prepare_for_depth_estimation(divided_result)
            
            # Th·ª±c hi·ªán depth estimation cho t·ª´ng v√πng
            for region in depth_regions:
                bbox = region['bbox']
                region_info = region['region_info']
                
                # ∆Ø·ªõc t√≠nh ƒë·ªô s√¢u cho bbox n√†y
                region_depth = depth_model.estimate_depth(frame, [bbox])
                
                # T·∫°o k·∫øt qu·∫£ chi ti·∫øt cho region
                if region_depth and len(region_depth) > 0:
                    depth_info = region_depth[0]  # L·∫•y k·∫øt qu·∫£ ƒë·∫ßu ti√™n
                    result = {
                        'region_info': region_info,
                        'bbox': bbox,
                        'center': region['center'],
                        'depth': depth_info,
                        'position': {
                            'x': region['center'][0],
                            'y': region['center'][1], 
                            'z': depth_info.get('mean_depth', 0.0) if isinstance(depth_info, dict) else 0.0
                        }
                    }
                    
                    # Th√™m corners n·∫øu c√≥ (ƒë·ªÉ v·∫Ω rotated boxes)
                    if 'corners' in region:
                        result['corners'] = region['corners']
                    
                    # Th√™m corners g·ªëc c·ªßa pallet n·∫øu c√≥
                    if 'original_corners' in region:
                        result['original_corners'] = region['original_corners']
                    
                    region_depth_results.append(result)
            
            # Gi·ªØ l·∫°i depth_results c≈© ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code hi·ªÉn th·ªã
            if region_depth_results:
                depth_results = [r['depth'] for r in region_depth_results]
            
            depth_end_time = time.time()
            depth_process_time = (depth_end_time - yolo_time) * 1000
            total_depth_time += depth_process_time
        
        total_process_time = yolo_process_time + depth_process_time
        total_time += total_process_time
        
        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        num_objects = len(detections['bounding_boxes'])
        print(f"  Th·ªùi gian YOLO: {yolo_process_time:.2f} ms")
        if depth_model.enable:
            print(f"  Th·ªùi gian Depth: {depth_process_time:.2f} ms")
            print(f"  T·ªïng th·ªùi gian: {total_process_time:.2f} ms")
        print(f"  ƒê√£ ph√°t hi·ªán: {num_objects} ƒë·ªëi t∆∞·ª£ng")
        
        if num_objects > 0:
            successful_detections += 1
        
        # Hi·ªÉn th·ªã th√¥ng tin depth n·∫øu c√≥
        if region_depth_results and len(region_depth_results) > 0:
            successful_depth_detections += 1
            print(f"  Th√¥ng tin ƒë·ªô s√¢u theo v√πng ({len(region_depth_results)} v√πng):")
            for j, result in enumerate(region_depth_results):
                region_info = result['region_info']
                depth_info = result['depth']
                position = result['position']
                
                pallet_id = region_info.get('pallet_id', 1)
                region_id = region_info.get('region_id', 1)
                layer = region_info.get('layer', 1)
                
                if isinstance(depth_info, dict):
                    mean_depth = depth_info.get('mean_depth', 0.0)
                else:
                    mean_depth = 0.0
                
                print(f"    P{pallet_id}R{region_id}L{layer}: {mean_depth:.2f}m")
        elif depth_results and len(depth_results) > 0:
            # Fallback cho tr∆∞·ªùng h·ª£p kh√¥ng c√≥ region results
            successful_depth_detections += 1
            print(f"  Th√¥ng tin ƒë·ªô s√¢u (kh√¥ng chia v√πng):")
            for j, result in enumerate(depth_results):
                if isinstance(result, dict):
                    print(f"    ƒê·ªëi t∆∞·ª£ng {j+1}: {result.get('mean_depth', 0.0):.2f}m")
                else:
                    print(f"    ƒê·ªëi t∆∞·ª£ng {j+1}: Kh√¥ng c√≥ th√¥ng tin depth")
        
        # L∆∞u k·∫øt qu·∫£ detection
        base_name = os.path.splitext(img_file)[0]
        detection_output_path = os.path.join(output_folder, f"result_{base_name}.jpg")
        cv2.imwrite(detection_output_path, detections["annotated_frame"])
        print(f"  ƒê√£ l∆∞u detection: {detection_output_path}")
        
        # L∆∞u k·∫øt qu·∫£ rotated boxes v·ªõi depth
        if detections['corners'] or detections['bounding_boxes']:
            depth_viz = draw_rotated_boxes_with_depth(frame, detections, depth_results)
            rotated_output_path = os.path.join(rotated_folder, f"rotated_depth_{base_name}.jpg")
            cv2.imwrite(rotated_output_path, depth_viz)
            print(f"  ƒê√£ l∆∞u rotated boxes v·ªõi depth: {rotated_output_path}")
            
            # L∆∞u depth regions n·∫øu c√≥ chia v√πng
            if region_depth_results:
                region_viz = draw_depth_regions_with_rotated_boxes(frame, region_depth_results)
                region_output_path = os.path.join(regions_folder, f"depth_regions_{base_name}.jpg")
                cv2.imwrite(region_output_path, region_viz)
                print(f"  ƒê√£ l∆∞u depth regions: {region_output_path}")
    
    # Th·ªëng k√™ t·ªïng k·∫øt
    print(f"\n=== TH·ªêNG K√ä T·ªîNG K·∫æT ===")
    print(f"T·ªïng s·ªë ·∫£nh x·ª≠ l√Ω: {len(image_files)}")
    print(f"·∫¢nh c√≥ ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng: {successful_detections}")
    print(f"T·ªâ l·ªá ph√°t hi·ªán th√†nh c√¥ng: {successful_detections/len(image_files)*100:.1f}%")
    
    if depth_model.enable:
        print(f"·∫¢nh c√≥ th√¥ng tin depth: {successful_depth_detections}")
        print(f"T·ªâ l·ªá depth th√†nh c√¥ng: {successful_depth_detections/len(image_files)*100:.1f}%")
        print(f"Th·ªùi gian YOLO trung b√¨nh: {total_yolo_time/len(image_files):.2f} ms/·∫£nh")
        print(f"Th·ªùi gian Depth trung b√¨nh: {total_depth_time/len(image_files):.2f} ms/·∫£nh")
    
    print(f"Th·ªùi gian t·ªïng trung b√¨nh: {total_time/len(image_files):.2f} ms/·∫£nh")
    print(f"K·∫øt qu·∫£ detection ƒë√£ ƒë∆∞·ª£c l∆∞u trong folder: {output_folder}")
    print(f"K·∫øt qu·∫£ rotated boxes v·ªõi depth ƒë√£ ƒë∆∞·ª£c l∆∞u trong folder: {rotated_folder}")
    print(f"K·∫øt qu·∫£ depth regions (Module Division) ƒë√£ ƒë∆∞·ª£c l∆∞u trong folder: {regions_folder}")

# Di chuy·ªÉn c√°c h√†m factory ra ngo√†i h√†m demo_camera ƒë·ªÉ c√≥ th·ªÉ pickle
def create_camera():
    camera = CameraInterface(camera_index=0)
    camera.initialize()
    return camera

def create_yolo():
    return YOLOTensorRT(engine_path=ENGINE_PATH, conf=0.55)

def create_depth():
    # Cho ph√©p ch·∫°y depth model tr√™n CPU ho·∫∑c t·∫Øt ho√†n to√†n
    use_device = os.environ.get('DEPTH_DEVICE', 'cuda')  # 'cuda', 'cpu' ho·∫∑c 'off' 
    enable_depth = use_device.lower() != 'off'
    
    # L·∫•y lo·∫°i model: regular ho·∫∑c metric
    model_type = os.environ.get('DEPTH_TYPE', 'metric').lower()  # 'regular' ho·∫∑c 'metric'
    
    # L·∫•y k√≠ch th∆∞·ªõc model
    model_size = os.environ.get('DEPTH_MODEL', 'small').lower()  # 'large', 'base', 'small'
    
    # L·∫•y lo·∫°i scene cho metric depth
    scene_type = os.environ.get('DEPTH_SCENE', 'indoor').lower()  # 'indoor' ho·∫∑c 'outdoor'
    
    # K√≠ch th∆∞·ªõc input
    input_size_str = os.environ.get('DEPTH_SIZE', '640x640')
    input_size = None
    if input_size_str:
        try:
            w, h = map(int, input_size_str.split('x'))
            input_size = (w, h)
        except:
            print(f"[Factory] Kh√¥ng th·ªÉ ph√¢n t√≠ch DEPTH_SIZE: {input_size_str}, s·ª≠ d·ª•ng k√≠ch th∆∞·ªõc g·ªëc")
    
    # B·ªè qua frame
    skip_frames_str = os.environ.get('DEPTH_SKIP', '50')
    try:
        skip_frames = int(skip_frames_str)
    except:
        skip_frames = 0
    
    # Camera calibration settings
    use_calibration = os.environ.get('USE_CAMERA_CALIBRATION', 'True').lower() in ('true', '1', 'yes')
    calibration_file = os.environ.get('CAMERA_CALIBRATION_FILE', 'camera_params.npz')
    
    if use_device.lower() == 'off':
        # print(f"[Factory] ƒê√£ t·∫Øt depth model ƒë·ªÉ ti·∫øt ki·ªám t√†i nguy√™n")
        return DepthEstimator(device='cpu', enable=False, use_camera_calibration=use_calibration, camera_calibration_file=calibration_file)
    
    # print(f"[Factory] Kh·ªüi t·∫°o depth model tr√™n thi·∫øt b·ªã: {use_device}")
    # print(f"[Factory] Model type: {model_type}, Size: {model_size}")
    # if model_type == 'metric':
    #     print(f"[Factory] Scene type: {scene_type}")
    # print(f"[Factory] Camera calibration: {'B·∫≠t' if use_calibration else 'T·∫Øt'}")
    # if use_calibration:
    #     print(f"[Factory] Calibration file: {calibration_file}")
    
    # T·∫°o DepthEstimator d·ª±a tr√™n lo·∫°i model
    if model_type == 'metric':
        return DepthEstimator.create_metric(
            scene_type=scene_type,
            model_size=model_size,
            device=use_device, 
            enable=enable_depth,
            input_size=input_size,
            skip_frames=skip_frames,
            use_camera_calibration=use_calibration,
            camera_calibration_file=calibration_file
        )
    else:
        return DepthEstimator.create_regular(
            model_size=model_size,
            device=use_device, 
            enable=enable_depth,
            input_size=input_size,
            skip_frames=skip_frames,
            use_camera_calibration=use_calibration,
            camera_calibration_file=calibration_file
        )

def demo_camera():
    """Th·ª≠ nghi·ªám v·ªõi camera th·ªùi gian th·ª±c"""
    # print("Th·ª≠ nghi·ªám ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng v·ªõi TensorRT tr√™n camera th·ªùi gian th·ª±c")
    global SHOW_DEPTH, SHOW_THETA4, SHOW_REGIONS
    
    # ‚≠ê PLC INTEGRATION SETUP ‚≠ê
    enable_plc = os.environ.get('ENABLE_PLC', 'true').lower() in ('true', '1', 'yes')
    plc_ip = os.environ.get('PLC_IP', '192.168.0.1')
    
    if enable_plc:
        print(f"üè≠ PLC Integration: ENABLED (IP: {plc_ip})")
        print("   üí° Nh·∫•n 'n' ƒë·ªÉ send regions v√†o PLC th·∫≠t s·ª±!")
    else:
        print(f"üè≠ PLC Integration: DISABLED")
        print("   üí° ƒê·ªÉ enable: set ENABLE_PLC=true")
        print("   üí° Set IP: set PLC_IP=192.168.1.100")
    
    # ‚≠ê GI·∫¢I TH√çCH C√ÅC TABS VISUALIZATION ‚≠ê
    print("üéØ H∆Ø·ªöNG D·∫™N C√ÅC TABS VISUALIZATION:")
    print("1. üì∫ TAB CH√çNH: 'Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng v·ªõi TensorRT'")
    print("   - Hi·ªÉn th·ªã: YOLO detection bounding boxes")
    print("   - Th√¥ng tin: FPS, s·ªë objects, theta4 success, robot coordinates")
    print("   - ƒê√¢y l√† tab lu√¥n hi·ªÉn th·ªã")
    print()
    print("2. üß† TAB THETA4: 'Theta4 Calculation & Regions' (Nh·∫•n 't' ƒë·ªÉ b·∫≠t/t·∫Øt)")
    print("   - Hi·ªÉn th·ªã: Theta4 rotation calculations")
    print("   - Bao g·ªìm: Load objects, pallet regions, rotation angles, mapping lines")
    print("   - C√≥ compass t·ªça ƒë·ªô v√† theta4 commands")
    print()
    print("3. üó∫Ô∏è TAB REGIONS: 'Region Processing & Detections' (Nh·∫•n 'r' ƒë·ªÉ b·∫≠t/t·∫Øt)")
    print("   - Hi·ªÉn th·ªã: Region boundaries (loads, pallets1, pallets2)")
    print("   - Bao g·ªìm: Detections ƒë∆∞·ª£c filter theo regions, unassigned objects")
    print("   - C√≥ statistics v·ªÅ s·ªë objects trong m·ªói region")
    print()
    print("4. üìè TAB DEPTH: 'Rotated Depth Regions' (Nh·∫•n 'd' ƒë·ªÉ b·∫≠t/t·∫Øt)")
    print("   - Hi·ªÉn th·ªã: Depth estimation v·ªõi rotated bounding boxes")
    print("   - Bao g·ªìm: Pallet regions (P1R1, P1R2, P1R3) v·ªõi depth values")
    print("   - Sequential regions ƒë∆∞·ª£c hi·ªÉn th·ªã ·ªü ƒë√¢y!")
    print()
    print("üîë QUAN TR·ªåNG: SEQUENTIAL REGIONS hi·ªÉn th·ªã ch·ªß y·∫øu ·ªü:")
    print("   üìè TAB DEPTH - N∆°i b·∫°n s·∫Ω th·∫•y P1R1, P1R2, P1R3 v·ªõi depth values")
    print("   üó∫Ô∏è TAB REGIONS - N∆°i b·∫°n th·∫•y region boundaries v√† filtering")
    print()
    print("‚å®Ô∏è KEYBOARD CONTROLS:")
    print("   'q': Tho√°t  |  'd': Depth tab  |  't': Theta4 tab  |  'r': Regions tab")
    print("   'h': Help (h∆∞·ªõng d·∫´n chi ti·∫øt)  |  's': Sequence status")
    print()
    
    # Hi·ªÉn th·ªã t√πy ch·ªçn depth, theta4 v√† regions
    # print(f"Hi·ªÉn th·ªã depth map: {'B·∫¨T' if SHOW_DEPTH else 'T·∫ÆT'} (D√πng 'd' ƒë·ªÉ b·∫≠t/t·∫Øt)")
    # print(f"Hi·ªÉn th·ªã theta4 info: {'B·∫¨T' if SHOW_THETA4 else 'T·∫ÆT'} (D√πng 't' ƒë·ªÉ b·∫≠t/t·∫Øt)")
    # print(f"Hi·ªÉn th·ªã regions: {'B·∫¨T' if SHOW_REGIONS else 'T·∫ÆT'} (D√πng 'r' ƒë·ªÉ b·∫≠t/t·∫Øt)")
    
    # Kh·ªüi t·∫°o pipeline v·ªõi c√°c factory function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü c·∫•p module
    pipeline = ProcessingPipeline(
        camera_factory=create_camera,
        yolo_factory=create_yolo,
        depth_factory=create_depth,
        enable_plc=enable_plc,
        plc_ip=plc_ip
    )
    
    # Bi·∫øn ƒë·ªÉ l∆∞u frame depth, theta4 v√† regions cu·ªëi c√πng
    last_depth_viz = None
    last_depth_time = 0
    last_theta4_viz = None
    last_theta4_time = 0
    last_regions_viz = None
    last_regions_time = 0
    skip_counter = 0
    max_skip = 5  # B·ªè qua t·ªëi ƒëa 5 frames khi x·ª≠ l√Ω kh√¥ng k·ªãp
    
    # Kh·ªüi ƒë·ªông pipeline
    if pipeline.start(timeout=60.0):
        print("Pipeline ƒë√£ kh·ªüi ƒë·ªông th√†nh c√¥ng!")
        
        # ‚≠ê CONNECT PLC IF ENABLED ‚≠ê
        if enable_plc:
            print(f"üîå ƒêang k·∫øt n·ªëi PLC...")
            plc_connected = pipeline.connect_plc()
            if plc_connected:
                print(f"‚úÖ K·∫øt n·ªëi PLC th√†nh c√¥ng!")
            else:
                print(f"‚ùå K·∫øt n·ªëi PLC th·∫•t b·∫°i! Ti·∫øp t·ª•c m√† kh√¥ng PLC...")
        
        # print("\nPh√≠m ƒëi·ªÅu khi·ªÉn:")
        # print("  'q': Tho√°t")
        # print("  'd': B·∫≠t/t·∫Øt hi·ªÉn th·ªã depth map")
        # print("  't': B·∫≠t/t·∫Øt hi·ªÉn th·ªã theta4 calculation")
        # print("  'r': B·∫≠t/t·∫Øt hi·ªÉn th·ªã regions processing")
        pass
        
        try:
            # V√≤ng l·∫∑p hi·ªÉn th·ªã k·∫øt qu·∫£
            fps_counter = 0
            fps_time = time.time()
            fps = 0.0  # Kh·ªüi t·∫°o FPS
            
            while True:
                start_loop = time.time()
                
                # L·∫•y k·∫øt qu·∫£ detection m·ªõi nh·∫•t
                detection_result = pipeline.get_latest_detection()
                if not detection_result:
                    # N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£ detection, ch·ªù m·ªôt ch√∫t
                    time.sleep(0.01)
                    continue
                
                frame, detections = detection_result
                
                # N·∫øu x·ª≠ l√Ω qu√° ch·∫≠m, tƒÉng skip_counter
                if time.time() - start_loop > 0.1:  # Qu√° 100ms
                    skip_counter += 1
                    if skip_counter >= max_skip:
                        # B·ªè qua hi·ªÉn th·ªã ƒë·ªÉ gi·∫£m t·∫£i
                        skip_counter = 0
                        continue
                else:
                    skip_counter = 0  # Reset n·∫øu x·ª≠ l√Ω nhanh
                
                # T√≠nh FPS
                fps_counter += 1
                if time.time() - fps_time >= 1.0:
                    fps = fps_counter / (time.time() - fps_time)
                    fps_counter = 0
                    fps_time = time.time()
                    
                    # C·∫≠p nh·∫≠t th√¥ng tin th·ªëng k√™
                    stats = pipeline.get_stats()
                # V·∫Ω FPS l√™n frame detection
                display_frame = detections["annotated_frame"].copy()
                
                # V·∫Ω FPS v·ªõi background ƒëen ƒë·ªÉ d·ªÖ ƒë·ªçc
                fps_text = f"FPS: {fps:.1f}"
                text_size = cv2.getTextSize(fps_text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)[0]
                cv2.rectangle(display_frame, 
                             (10 - 5, 30 - text_size[1] - 5),
                             (10 + text_size[0] + 5, 30 + 5),
                             (0, 0, 0), -1)
                cv2.putText(display_frame, fps_text, (10, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                
                # Th√™m th√¥ng tin s·ªë objects
                num_objects = len(detections.get('bounding_boxes', []))
                objects_text = f"Objects: {num_objects}"
                cv2.putText(display_frame, objects_text, (10, 70), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                
                # Th√™m th√¥ng tin theta4 n·∫øu c√≥
                theta4_result = detections.get('theta4_result')
                if theta4_result:
                    theta4_success = theta4_result['summary']['successful_theta4']
                    total_loads = theta4_result['summary']['total_loads']
                    theta4_text = f"Theta4: {theta4_success}/{total_loads}"
                    cv2.putText(display_frame, theta4_text, (10, 110), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)
                
                # ‚≠ê HI·ªÇN TH·ªä ROBOT COORDINATES ‚≠ê
                robot_coords = detections.get('robot_coordinates', [])
                if robot_coords:
                    coords_text = f"Robot Coords: {len(robot_coords)}"
                    cv2.putText(display_frame, coords_text, (10, 150), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
                
                # ‚≠ê DEBUG: KI·ªÇM TRA PALLET REGIONS DATA ‚≠ê
                pallet_regions = detections.get('pallet_regions', [])
                if pallet_regions:
                    regions_text = f"Pallet Regions: {len(pallet_regions)}"
                    cv2.putText(display_frame, regions_text, (10, 190), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 255), 2)
                    
                    # Debug log (m·ªói 10 frames ƒë·ªÉ sync v·ªõi pipeline)
                    if fps_counter % 10 == 0:
                        print(f"[DEBUG] Received pallet_regions: {len(pallet_regions)}")
                        for i, region in enumerate(pallet_regions):
                            region_info = region.get('region_info', {})
                            bbox = region.get('bbox', [])
                            corners = region.get('corners', [])
                            print(f"  Region {i}: P{region_info.get('pallet_id')}R{region_info.get('region_id')} bbox={[int(x) for x in bbox]} corners={len(corners) > 0}")
                else:
                    # Hi·ªÉn th·ªã th√¥ng b√°o kh√¥ng c√≥ regions
                    if fps_counter % 10 == 0:
                        print(f"[DEBUG] No pallet_regions data received")
                    
                    # In ra console cho t·ª´ng object - LOG ROBOT COORDINATES (m·ªói 10 frames)
                    if len(robot_coords) > 0 and fps_counter % 10 == 0:
                        print(f"[ROBOT COORDS] Frame {fps_counter}: {len(robot_coords)} objects")
                        for coord in robot_coords:
                            class_name = coord['class']
                            pixel = coord['camera_pixel']
                            robot_pos = coord['robot_coordinates']
                            cam_3d = coord.get('camera_3d')
                            
                            print(f"   {class_name}: Pixel({pixel['x']},{pixel['y']}) ‚Üí Robot(X={robot_pos['x']:.2f}, Y={robot_pos['y']:.2f})")
                            if cam_3d:
                                print(f"      (Camera3D: X={cam_3d['X']:.3f}, Y={cam_3d['Y']:.3f}, Z={cam_3d['Z']:.3f})")
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£ detection v·ªõi FPS
                cv2.imshow("Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng v·ªõi TensorRT", display_frame)
                
                # X·ª≠ l√Ω depth ch·ªâ khi SHOW_DEPTH ƒë∆∞·ª£c b·∫≠t
                if SHOW_DEPTH:
                    # Ch·ªâ l·∫•y depth m·ªõi sau m·ªói 0.5 gi√¢y
                    if time.time() - last_depth_time > 0.5:
                        depth_result = pipeline.get_latest_depth()
                        if depth_result:
                            frame_depth, depth_results = depth_result
                            
                            # S·ª≠ d·ª•ng helper function ƒë·ªÉ v·∫Ω rotated boxes v·ªõi depth
                            depth_viz = draw_depth_regions_with_rotated_boxes(frame_depth, depth_results)
                            
                            # L∆∞u l·∫°i ƒë·ªÉ t√°i s·ª≠ d·ª•ng
                            last_depth_viz = depth_viz
                            last_depth_time = time.time()
                    
                    # Hi·ªÉn th·ªã depth t·ª´ l·∫ßn x·ª≠ l√Ω g·∫ßn nh·∫•t
                    if last_depth_viz is not None:
                        cv2.imshow("Rotated Depth Regions", last_depth_viz)
                
                # ‚≠ê X·ª¨ L√ù THETA4 CH·ªà KHI SHOW_THETA4 ƒê∆Ø·ª¢C B·∫¨T ‚≠ê
                if SHOW_THETA4:
                    # Ch·ªâ c·∫≠p nh·∫≠t theta4 visualization sau m·ªói 0.3 gi√¢y ƒë·ªÉ tr√°nh lag
                    if time.time() - last_theta4_time > 0.3:
                        theta4_result = detections.get('theta4_result')
                        if theta4_result:
                            # S·ª≠ d·ª•ng function theta4 visualization
                            theta4_viz = draw_theta4_visualization(frame, detections)
                            
                            # L∆∞u l·∫°i ƒë·ªÉ t√°i s·ª≠ d·ª•ng
                            last_theta4_viz = theta4_viz
                            last_theta4_time = time.time()
                    
                    # Hi·ªÉn th·ªã theta4 t·ª´ l·∫ßn x·ª≠ l√Ω g·∫ßn nh·∫•t
                    if last_theta4_viz is not None:
                        cv2.imshow("Theta4 Calculation & Regions", last_theta4_viz)
                
                # ‚≠ê X·ª¨ L√ù REGIONS CH·ªà KHI SHOW_REGIONS ƒê∆Ø·ª¢C B·∫¨T ‚≠ê
                if SHOW_REGIONS:
                    # Ch·ªâ c·∫≠p nh·∫≠t regions visualization sau m·ªói 0.2 gi√¢y ƒë·ªÉ tr√°nh lag
                    if time.time() - last_regions_time > 0.2:
                        region_filtered = detections.get('region_filtered')
                        if region_filtered:
                            # S·ª≠ d·ª•ng function regions visualization
                            regions_viz = draw_region_visualization(frame, detections)
                            
                            # L∆∞u l·∫°i ƒë·ªÉ t√°i s·ª≠ d·ª•ng
                            last_regions_viz = regions_viz
                            last_regions_time = time.time()
                    
                    # Hi·ªÉn th·ªã regions t·ª´ l·∫ßn x·ª≠ l√Ω g·∫ßn nh·∫•t
                    if last_regions_viz is not None:
                        cv2.imshow("Region Processing & Detections", last_regions_viz)
                
                # X·ª≠ l√Ω ph√≠m nh·∫•n
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('d'):
                    # B·∫≠t/t·∫Øt hi·ªÉn th·ªã depth
                    SHOW_DEPTH = not SHOW_DEPTH
                    # print(f"Hi·ªÉn th·ªã depth map: {'B·∫¨T' if SHOW_DEPTH else 'T·∫ÆT'}")
                    if not SHOW_DEPTH:
                        cv2.destroyWindow("Rotated Depth Regions")
                elif key == ord('t'):
                    # B·∫≠t/t·∫Øt hi·ªÉn th·ªã theta4
                    SHOW_THETA4 = not SHOW_THETA4
                    # print(f"Hi·ªÉn th·ªã theta4 calculation: {'B·∫¨T' if SHOW_THETA4 else 'T·∫ÆT'}")
                    if not SHOW_THETA4:
                        cv2.destroyWindow("Theta4 Calculation & Regions")
                elif key == ord('h'):
                    # Show help
                    print("\nüìö [HELP] H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG:")
                    print("=== KEYBOARD CONTROLS ===")
                    print("'q': Tho√°t")
                    print("'d': B·∫≠t/t·∫Øt TAB DEPTH (üìè 'Rotated Depth Regions')")
                    print("'t': B·∫≠t/t·∫Øt TAB THETA4 (üß† 'Theta4 Calculation & Regions')")
                    print("'r': B·∫≠t/t·∫Øt TAB REGIONS (üó∫Ô∏è 'Region Processing & Detections')")
                    print("'h': Hi·ªÉn th·ªã help n√†y")
                    print()
                    print("=== SEQUENTIAL CONTROLS ===")
                    if enable_plc:
                        print("'n': ‚≠ê SEND REGIONS TO PLC ‚≠ê (Real PLC sending)")
                    else:
                        print("'n': Next region (demo only - enable PLC to send real data)")
                    print("'c': Complete region (demo only)")
                    print("'s': Show sequence status")
                    print("'x': Reset sequence (demo only)")
                    print("'z': Show depth info")
                    print()
                    print("=== PLC INTEGRATION ===")
                    print(f"PLC Status: {'üü¢ ENABLED' if enable_plc else 'üî¥ DISABLED'}")
                    if enable_plc:
                        plc_integration = pipeline.get_plc_integration()
                        if plc_integration and plc_integration.plc_connected:
                            print(f"PLC Connection: ‚úÖ CONNECTED (IP: {plc_ip})")
                        else:
                            print(f"PLC Connection: ‚ùå DISCONNECTED (IP: {plc_ip})")
                    else:
                        print("To enable: set ENABLE_PLC=true")
                    print()
                    print("=== TABS VISUALIZATION ===")
                    print("üì∫ TAB CH√çNH: Lu√¥n hi·ªÉn th·ªã YOLO detections")
                    print("üìè TAB DEPTH: Sequential regions P1R1, P1R2, P1R3")
                    print("üó∫Ô∏è TAB REGIONS: Region boundaries & filtering")
                    print("üß† TAB THETA4: Rotation calculations")
                    print()
                    print("üí° QUAN TR·ªåNG:")
                    print("- Sequential regions hi·ªÉn th·ªã ch·ªß y·∫øu ·ªü TAB DEPTH")
                    print("- Sequential logic ch·∫°y t·ª± ƒë·ªông khi c√≥ pallets & loads")
                    print("- Keyboard controls ch·ªâ l√† demo, actual logic ch·∫°y t·ª± ƒë·ªông")
                        
                # ‚≠ê SEQUENTIAL REGION CONTROLS - ADDED FOR PLAN IMPLEMENTATION ‚≠ê
                elif key == ord('n'):
                    # ‚≠ê SEND REGIONS TO PLC ‚≠ê
                    print("\nüöÄ [PLC SENDING] Manual trigger: Send regions to PLC...")
                    print(f"üè≠ PLC Integration enabled: {enable_plc}")
                    
                    if not enable_plc:
                        print("   ‚ùå PLC not enabled! Set ENABLE_PLC=true to enable.")
                        continue
                    
                    try:
                        # ‚≠ê STEP 1: DIRECT PLC ACCESS (PRIMARY METHOD) ‚≠ê
                        print("   üéØ Using direct PLC access method...")
                        plc_integration = pipeline.get_plc_integration()
                        
                        if not plc_integration:
                            print("   ‚ùå PLC integration not available!")
                            continue
                        
                        if not plc_integration.plc_connected:
                            print("   üîå PLC not connected, attempting to connect...")
                            connected = plc_integration.connect_plc()
                            if not connected:
                                print("   ‚ùå Failed to connect to PLC!")
                                continue
                            else:
                                print("   ‚úÖ PLC connected successfully!")
                        
                        # ‚≠ê STEP 2: GET CURRENT FRAME DATA ‚≠ê
                        print("   üì° Getting current frame data...")
                        
                        # Try multiple times with short intervals to catch current frame
                        detection_result = None
                        for attempt in range(5):
                            detection_result = pipeline.get_latest_detection(timeout=0.1)
                            if detection_result:
                                break
                            print(f"      Attempt {attempt+1}/5: Waiting for frame...")
                            time.sleep(0.05)  # Short wait
                        
                        if not detection_result:
                            print("   ‚ö†Ô∏è No current frame data available, using last known data...")
                            # Use any available regions from pipeline state
                            if hasattr(plc_integration, 'last_regions_data') and plc_integration.last_regions_data:
                                print("   üìã Using cached regions data from PLC integration...")
                                success = plc_integration.send_regions_to_plc()
                                if success:
                                    print("   ‚úÖ Successfully sent cached regions to PLC!")
                                    
                                    # Show BAG PALLET STATUS
                                    bag_status = plc_integration.get_bag_pallet_status()
                                    print(f"   üì¶ BAG PALLET TRACKING:")
                                    print(f"      bag_pallet_1 = {bag_status['bag_pallet_1']}")
                                    print(f"      bag_pallet_2 = {bag_status['bag_pallet_2']}")
                                    print(f"      Active regions: {bag_status['active_regions_count']}")
                                    
                                    # Show regions
                                    for region_name, region_data in bag_status['current_regions'].items():
                                        if region_data:
                                            pallet_id = region_data['pallet_id']
                                            region_id = region_data['region_id']
                                            robot_coords = region_data['robot_coords']
                                            print(f"      {region_name}: P{pallet_id}R{region_id} ‚Üí Px={robot_coords['px']:.2f}, Py={robot_coords['py']:.2f}")
                                            
                                    # READ BACK FROM PLC
                                    print("   üîç Reading back from PLC to verify...")
                                    plc_data = plc_integration.read_regions_from_plc()
                                    if plc_data:
                                        print("   üìä PLC Memory Content:")
                                        for region_name, data in plc_data.items():
                                            print(f"      {region_name}: Px={data['px']:.2f} (DB26.{data['px_offset']}), Py={data['py']:.2f} (DB26.{data['py_offset']})")
                                else:
                                    print("   ‚ùå Failed to send cached regions to PLC")
                            else:
                                print("   ‚ùå No cached regions data available")
                            continue
                        
                        # ‚≠ê STEP 3: PROCESS AND SEND TO PLC ‚≠ê
                        frame, detections = detection_result
                        print("   üîÑ Processing current frame for PLC sending...")
                        
                        regions_data, success = plc_integration.process_detection_and_send_to_plc(detections, layer=1)
                        
                        if success:
                            print("   ‚úÖ Successfully sent regions to PLC!")
                            print(f"   üìã Processed {len(regions_data)} regions")
                            
                            # ‚≠ê SHOW BAG PALLET TRACKING STATUS ‚≠ê
                            bag_status = plc_integration.get_bag_pallet_status()
                            print(f"   üì¶ BAG PALLET TRACKING:")
                            print(f"      bag_pallet_1 = {bag_status['bag_pallet_1']}")
                            print(f"      bag_pallet_2 = {bag_status['bag_pallet_2']}")
                            print(f"      Active regions: {bag_status['active_regions_count']}")
                            
                            # ‚≠ê SHOW DETAILED REGION DATA ‚≠ê
                            for region_name, region_data in bag_status['current_regions'].items():
                                if region_data:
                                    pallet_id = region_data['pallet_id']
                                    region_id = region_data['region_id']
                                    robot_coords = region_data['robot_coords']
                                    print(f"      {region_name}: P{pallet_id}R{region_id} ‚Üí Px={robot_coords['px']:.2f}, Py={robot_coords['py']:.2f}")
                            
                            # ‚≠ê READ BACK FROM PLC TO VERIFY ‚≠ê
                            print("   üîç Reading back from PLC to verify...")
                            plc_data = plc_integration.read_regions_from_plc()
                            if plc_data:
                                print("   üìä PLC Memory Content:")
                                for region_name, data in plc_data.items():
                                    print(f"      {region_name}: Px={data['px']:.2f} (DB26.{data['px_offset']}), Py={data['py']:.2f} (DB26.{data['py_offset']})")
                        else:
                            print("   ‚ùå Failed to send regions to PLC")
                            if regions_data:
                                print(f"   üìã Regions were processed ({len(regions_data)}) but PLC sending failed")
                        
                    except Exception as e:
                        print(f"   ‚ùå Error in PLC sending process: {e}")
                        import traceback
                        traceback.print_exc()
                        
                elif key == ord('c'):
                    # Complete current region (robot ho√†n th√†nh, chuy·ªÉn sang region ti·∫øp theo)
                    print("\n‚úÖ [SEQUENCE] Robot completed current region...")
                    try:
                        sequencer = pipeline.get_region_sequencer()
                        if sequencer and sequencer.is_available():
                            sequencer.mark_region_completed()
                            # SequencerProxy s·∫Ω explain v·ªÅ manual completion
                        else:
                            print("   ‚ö†Ô∏è RegionSequencer ƒëang kh·ªüi t·∫°o ho·∫∑c ch∆∞a c√≥ detections")
                            print("   üí° H√£y ch·ªù m·ªôt ch√∫t ƒë·ªÉ system ph√°t hi·ªán pallets")
                    except Exception as e:
                        print(f"   ‚ùå Error: {e}")
                        
                elif key == ord('s'):
                    # Show sequence status
                    print("\nüìä [SEQUENCE] Current Status:")
                    try:
                        sequencer = pipeline.get_region_sequencer()
                        if sequencer and sequencer.is_available():
                            status = sequencer.get_queue_status()
                            print(f"   Status: {status['status']}")
                            if status['current_pallet']:
                                print(f"   Current Pallet: P{status['current_pallet']}")
                            print(f"   Progress: {status['progress']}")
                            print(f"   Sequence: {status['sequence']}")
                            print(f"   Completed: {status['completed_count']}")
                            print(f"   Remaining: {status['remaining_count']}")
                            
                            # Show remaining regions
                            if status['remaining_regions']:
                                print("   Remaining regions:")
                                for region in status['remaining_regions']:
                                    marker = "‚Üí " if region.get('is_current') else "  "
                                    print(f"     {marker}Region {region.get('region_id', '?')} (seq {region.get('sequence_order', '?')})")
                            
                            print("   ‚ÑπÔ∏è Sequential sending ƒëang ch·∫°y t·ª± ƒë·ªông trong background")
                        else:
                            print("   ‚ö†Ô∏è RegionSequencer ƒëang kh·ªüi t·∫°o ho·∫∑c ch∆∞a c√≥ detections")
                            print("   üí° H√£y ch·ªù m·ªôt ch√∫t ƒë·ªÉ system ph√°t hi·ªán pallets")
                    except Exception as e:
                        print(f"   ‚ùå Error: {e}")
                        
                elif key == ord('x'):
                    # Reset sequence
                    print("\nüîÑ [SEQUENCE] Resetting sequence...")
                    try:
                        sequencer = pipeline.get_region_sequencer()
                        if sequencer and sequencer.is_available():
                            sequencer.reset_sequence()
                            # SequencerProxy s·∫Ω explain v·ªÅ manual reset
                        else:
                            print("   ‚ö†Ô∏è RegionSequencer ƒëang kh·ªüi t·∫°o ho·∫∑c ch∆∞a c√≥ detections")
                            print("   üí° H√£y ch·ªù m·ªôt ch√∫t ƒë·ªÉ system ph√°t hi·ªán pallets")
                    except Exception as e:
                        print(f"   ‚ùå Error: {e}")
                        
                elif key == ord('z'):
                    # Show depth info (Z values)
                    print("\nüîç [DEPTH INFO] Current depth information:")
                    try:
                        depth_result = pipeline.get_latest_depth()
                        if depth_result:
                            frame_depth, depth_results = depth_result
                            print(f"   Found {len(depth_results)} depth regions:")
                            for i, region in enumerate(depth_results):
                                region_info = region.get('region_info', {})
                                position = region.get('position', {})
                                z_value = position.get('z', 0.0)
                                pallet_id = region_info.get('pallet_id', 0)
                                region_id = region_info.get('region_id', 0)
                                
                                if pallet_id > 0:
                                    print(f"     P{pallet_id}R{region_id}: Z={z_value:.3f}m")
                                else:
                                    obj_class = region_info.get('object_class', 'Unknown')
                                    print(f"     {obj_class}: Z={z_value:.3f}m")
                        else:
                            print("   ‚ö†Ô∏è No depth results available")
                    except Exception as e:
                        print(f"   ‚ùå Error: {e}")
                elif key == ord('r'):
                    # B·∫≠t/t·∫Øt hi·ªÉn th·ªã regions
                    SHOW_REGIONS = not SHOW_REGIONS
                    # print(f"Hi·ªÉn th·ªã regions: {'B·∫¨T' if SHOW_REGIONS else 'T·∫ÆT'}")
                    if not SHOW_REGIONS:
                        cv2.destroyWindow("Region Processing & Detections")
                        
        except KeyboardInterrupt:
            # print("ƒê√£ nh·∫≠n t√≠n hi·ªáu ng·∫Øt t·ª´ b√†n ph√≠m")
            pass
        finally:
            # D·ª´ng pipeline
            pipeline.stop()
            cv2.destroyAllWindows()
            # print("Pipeline ƒë√£ d·ª´ng")
    else:
        # print("Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông pipeline!")
        # Ki·ªÉm tra l·ªói
        for error in pipeline.errors:
            print(f"L·ªói: {error}")

def demo_sequential_region_sending():
    """Demo Sequential Region Sending v·ªõi BAG PALLET TRACKING"""
    print("üöÄ Demo Sequential Region Sending v·ªõi BAG PALLET TRACKING")
    print("H·ªá th·ªëng s·∫Ω:")
    print("1. Ph√°t hi·ªán pallets v√† x√°c ƒë·ªãnh workspace region (pallets1/pallets2)")
    print("2. Chia pallets th√†nh 3 regions cho t·ª´ng pallet")
    print("3. G·ª≠i t·ª´ng region m·ªôt theo th·ª© t·ª±, ch·ªù robot ho√†n th√†nh")
    print("4. Theo d√µi bag pallet tracking (load n√†o g·∫Øp v√†o pallet n√†o)")
    print()
    
    # Kh·ªüi t·∫°o components
    model = YOLOTensorRT(engine_path=ENGINE_PATH, conf=0.25)
    region_plc = RegionDivisionPLCIntegration(debug=True)
    
    # Test mode (kh√¥ng c·∫ßn k·∫øt n·ªëi PLC th·∫≠t)
    use_mock_plc = input("S·ª≠ d·ª•ng mock PLC? (y/n, m·∫∑c ƒë·ªãnh y): ").lower()
    if use_mock_plc != 'n':
        print("üîß S·ª≠ d·ª•ng mock PLC mode cho demo")
        region_plc.plc_connected = True  # Mock connection
        
        # Mock write function
        def mock_write_db26_real(offset, value):
            print(f"    [MOCK PLC] Write DB26.{offset} = {value:.2f}")
            return True
        region_plc.plc_comm.write_db26_real = mock_write_db26_real
    else:
        # K·∫øt n·ªëi PLC th·∫≠t
        region_plc.connect_plc()
    
    if not region_plc.plc_connected:
        print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi PLC, tho√°t demo")
        return
    
    # Demo v·ªõi camera ho·∫∑c ·∫£nh
    input_mode = input("Ch·ªçn input: (1) Camera real-time, (2) ·∫¢nh t·ª´ folder (m·∫∑c ƒë·ªãnh 2): ")
    
    if input_mode == "1":
        demo_sequential_with_camera(model, region_plc)
    else:
        demo_sequential_with_images(model, region_plc)

def demo_sequential_with_camera(model, region_plc):
    """Demo sequential sending v·ªõi camera real-time"""
    print("\nüìπ Demo v·ªõi camera real-time")
    print("Ph√≠m ƒëi·ªÅu khi·ªÉn:")
    print("  'q': Tho√°t")
    print("  'n': G·ª≠i region ti·∫øp theo (manual)")
    print("  'c': Robot ho√†n th√†nh (complete signal)")
    print("  's': Hi·ªÉn th·ªã status")
    print("  'a': B·∫≠t/t·∫Øt auto sending")
    
    camera = CameraInterface(camera_index=0)
    if not camera.initialize():
        print("‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o camera")
        return
    
    try:
        frame_count = 0
        last_detection_time = 0
        
        while True:
            ret, frame = camera.read()
            if not ret:
                print("‚ùå Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ camera")
                break
            
            frame_count += 1
            current_time = time.time()
            
            # Ch·ªâ detect m·ªói 2 gi√¢y ƒë·ªÉ tr√°nh spam
            if current_time - last_detection_time > 2.0:
                print(f"\nüîç Frame {frame_count}: ƒêang ph√°t hi·ªán...")
                
                # YOLO detection
                detections = model.detect(frame)
                num_objects = len(detections.get('bounding_boxes', []))
                
                if num_objects > 0:
                    print(f"   Ph√°t hi·ªán {num_objects} objects")
                    
                    # X·ª≠ l√Ω sequential region sending
                    regions_data, _ = region_plc.process_detection_and_send_to_plc(detections, layer=1)
                    
                    if regions_data:
                        print(f"   ƒê√£ t·∫°o {len(regions_data)} regions v√† organize v√†o queues")
                    
                    last_detection_time = current_time
                else:
                    print(f"   Kh√¥ng ph√°t hi·ªán objects n√†o")
            
            # T·∫°o visualization
            if region_plc.last_regions_data:
                display_frame = region_plc.create_visualization(frame)
            else:
                display_frame = frame.copy()
                # V·∫Ω th√¥ng tin status c∆° b·∫£n
                cv2.putText(display_frame, "Sequential Region Sending Demo", (10, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
                cv2.putText(display_frame, "Waiting for detections...", (10, 70), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            
            cv2.imshow("Sequential Region Sending Demo", display_frame)
            
            # X·ª≠ l√Ω ph√≠m
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('n'):
                # Manual send next region
                print("\nü§ñ Manual trigger: G·ª≠i region ti·∫øp theo...")
                success = region_plc.manual_send_next()
                print(f"   Result: {'‚úÖ Success' if success else '‚ùå Failed'}")
            elif key == ord('c'):
                # Robot completed signal
                print("\nü§ñ‚úÖ Robot completed signal...")
                success = region_plc.robot_completed_signal()
                print(f"   Next send result: {'‚úÖ Success' if success else '‚ùå No more regions'}")
            elif key == ord('s'):
                # Show status
                status = region_plc.get_queue_status()
                print(f"\nüìä SYSTEM STATUS:")
                print(f"   Robot: {status['robot_status']}")
                print(f"   Queues: pallets1={status['queue_counts']['pallets1']}, pallets2={status['queue_counts']['pallets2']}")
                print(f"   Total pending: {status['total_pending']}")
                print(f"   Stats: Sent {status['stats']['total_regions_sent']} regions")
                print(f"   BAG Tracking: pallet1={status['bag_pallet_1']}, pallet2={status['bag_pallet_2']}")
                if status['current_sending_region']:
                    current = status['current_sending_region']
                    print(f"   Current sending: {current['workspace_region']} P{current['pallet_id']}R{current['region_id']}")
            elif key == ord('a'):
                # Toggle auto sending
                region_plc.auto_send_enabled = not region_plc.auto_send_enabled
                print(f"\nüîÑ Auto sending: {'ENABLED' if region_plc.auto_send_enabled else 'DISABLED'}")
            
    finally:
        camera.release()
        cv2.destroyAllWindows()
        region_plc.disconnect_plc()

def demo_sequential_with_images(model, region_plc):
    """Demo sequential sending v·ªõi ·∫£nh t·ª´ folder"""
    print("\nüñºÔ∏è Demo v·ªõi ·∫£nh t·ª´ folder")
    
    pallets_folder = "images_pallets2"
    if not os.path.exists(pallets_folder):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y folder {pallets_folder}")
        return
    
    # L·∫•y danh s√°ch ·∫£nh
    image_files = [f for f in os.listdir(pallets_folder) if f.endswith(('.jpg', '.jpeg', '.png'))]
    image_files.sort()
    
    if not image_files:
        print("‚ùå Kh√¥ng c√≥ ·∫£nh n√†o trong folder")
        return
    
    print(f"üìÅ T√¨m th·∫•y {len(image_files)} ·∫£nh")
    for i, img_file in enumerate(image_files, 1):
        print(f"  {i}. {img_file}")
    
    try:
        for i, img_file in enumerate(image_files, 1):
            image_path = os.path.join(pallets_folder, img_file)
            print(f"\n[{i}/{len(image_files)}] üîç X·ª≠ l√Ω: {img_file}")
            
            frame = cv2.imread(image_path)
            if frame is None:
                print(f"   ‚ùå Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {img_file}")
                continue
            
            # YOLO detection
            detections = model.detect(frame)
            num_objects = len(detections.get('bounding_boxes', []))
            
            print(f"   Ph√°t hi·ªán {num_objects} objects")
            
            if num_objects > 0:
                # X·ª≠ l√Ω sequential region sending
                regions_data, _ = region_plc.process_detection_and_send_to_plc(detections, layer=1)
                
                if regions_data:
                    print(f"   ‚úÖ ƒê√£ t·∫°o {len(regions_data)} regions")
                    
                    # Show detailed regions
                    for region in regions_data:
                        workspace_region = region['workspace_region']
                        pallet_id = region['pallet_id']
                        region_id = region['region_id']
                        robot_coords = region['robot_coordinates']
                        print(f"      {workspace_region} P{pallet_id}R{region_id}: Px={robot_coords['px']:.2f}, Py={robot_coords['py']:.2f}")
                    
                    # Hi·ªÉn th·ªã visualization
                    visualization = region_plc.create_visualization(frame)
                    
                    cv2.imshow(f"Sequential Demo - {img_file}", visualization)
                    cv2.imshow(f"Original Detection - {img_file}", detections["annotated_frame"])
                    
                    print(f"   üìã Queue status:")
                    status = region_plc.get_queue_status()
                    for workspace_region, count in status['queue_counts'].items():
                        if count > 0:
                            print(f"      {workspace_region}: {count} regions pending")
                    
                    print(f"   ü§ñ Robot status: {status['robot_status']}")
                    if status['current_sending_region']:
                        current = status['current_sending_region']
                        print(f"   ‚ö° Currently sending: {current['workspace_region']} P{current['pallet_id']}R{current['region_id']}")
                    
                    print(f"\n   Nh·∫•n ph√≠m ƒë·ªÉ ƒëi·ªÅu khi·ªÉn:")
                    print(f"     'n': Send next region")
                    print(f"     'c': Robot completed")
                    print(f"     'space': Next image")
                    print(f"     'q': Quit")
                    
                    # Wait for user input
                    while True:
                        key = cv2.waitKey(0) & 0xFF
                        if key == ord('n'):
                            # Send next
                            success = region_plc.manual_send_next()
                            print(f"      {'‚úÖ Sent next region' if success else '‚ùå No more regions to send'}")
                            if success:
                                new_status = region_plc.get_queue_status()
                                if new_status['current_sending_region']:
                                    current = new_status['current_sending_region']
                                    print(f"      Now sending: {current['workspace_region']} P{current['pallet_id']}R{current['region_id']}")
                        elif key == ord('c'):
                            # Robot completed
                            success = region_plc.robot_completed_signal()
                            print(f"      ü§ñ‚úÖ Robot completed. {'Next region sent' if success else 'All regions completed'}")
                            if success:
                                new_status = region_plc.get_queue_status()
                                if new_status['current_sending_region']:
                                    current = new_status['current_sending_region']
                                    print(f"      Now sending: {current['workspace_region']} P{current['pallet_id']}R{current['region_id']}")
                        elif key == ord(' '):
                            # Next image
                            break
                        elif key == ord('q'):
                            cv2.destroyAllWindows()
                            return
                else:
                    print(f"   ‚ùå Kh√¥ng t·∫°o ƒë∆∞·ª£c regions")
            else:
                print(f"   ‚ö†Ô∏è Kh√¥ng c√≥ objects ƒë·ªÉ x·ª≠ l√Ω")
                
                # Hi·ªÉn th·ªã ·∫£nh g·ªëc
                cv2.imshow(f"No Detection - {img_file}", frame)
                key = cv2.waitKey(0) & 0xFF
                if key == ord('q'):
                    break
            
            cv2.destroyAllWindows()
    
    finally:
        region_plc.disconnect_plc()
        print(f"\nüìä FINAL STATISTICS:")
        final_status = region_plc.get_queue_status()
        print(f"   Total regions processed: {final_status['stats']['total_regions_detected']}")
        print(f"   Total regions sent: {final_status['stats']['total_regions_sent']}")
        print(f"   Pallets1 sent: {final_status['stats']['pallets1_sent']}")
        print(f"   Pallets2 sent: {final_status['stats']['pallets2_sent']}")
        print(f"   Final BAG tracking: pallet1={final_status['bag_pallet_1']}, pallet2={final_status['bag_pallet_2']}")

if __name__ == "__main__":
    print("Demo s·ª≠ d·ª•ng model TensorRT v·ªõi Rotated Bounding Boxes, Module Division, Theta4 Calculation v√† Region Processing")
    print("1. Th·ª≠ nghi·ªám v·ªõi ·∫£nh ƒë∆°n l·∫ª (c√≥ Module Division + depth estimation)")
    print("2. Th·ª≠ nghi·ªám v·ªõi camera th·ªùi gian th·ª±c (c√≥ Module Division + depth estimation + Theta4 + Regions)")
    print("3. Th·ª≠ nghi·ªám v·ªõi t·∫•t c·∫£ ·∫£nh trong folder images_pallets2 (c√≥ Module Division + depth estimation)")
    print("4. üöÄ Sequential Region Sending v·ªõi BAG PALLET TRACKING (T√≠nh nƒÉng m·ªõi!)")
    print("\nT√çNH NƒÇNG M·ªöI - SEQUENTIAL REGION SENDING (Option 4):")
    print("- Ph√°t hi·ªán pallets v√† x√°c ƒë·ªãnh workspace region (pallets1/pallets2)")  
    print("- Chia pallets th√†nh 3 regions v√† g·ª≠i t·ª´ng region m·ªôt theo th·ª© t·ª±")
    print("- BAG PALLET TRACKING: Theo d√µi load n√†o g·∫Øp v√†o pallet n√†o")
    print("- Sequential sending: Ch·ªù robot ho√†n th√†nh tr∆∞·ªõc khi g·ª≠i region ti·∫øp theo")
    print("- PLC Communication v·ªõi DB26 offsets ri√™ng cho t·ª´ng workspace region")
    print("\nT√çNH NƒÇNG REGION PROCESSING:")
    print("- Chia kh√¥ng gian l√†m vi·ªác th√†nh 3 regions c·ªë ƒë·ªãnh: loads, pallets1, pallets2")  
    print("- M·ªói region c√≥ th·ªÉ c√≥ offset ri√™ng cho robot coordinates")
    print("- Detections ƒë∆∞·ª£c filter v√† group theo regions")
    print("- S·ª≠ d·ª•ng ph√≠m 'r' ƒë·ªÉ b·∫≠t/t·∫Øt hi·ªÉn th·ªã regions trong real-time")
    print("\nT√çNH NƒÇNG THETA4 CALCULATION:")
    print("- Ch·∫ø ƒë·ªô camera (2) hi·ªán c√≥ t√≠nh to√°n g√≥c xoay theta4 cho robot")
    print("- S·ª≠ d·ª•ng ph√≠m 't' ƒë·ªÉ b·∫≠t/t·∫Øt hi·ªÉn th·ªã theta4 trong real-time")
    print("- Theta4 s·∫Ω hi·ªÉn th·ªã g√≥c c·∫ßn xoay cho t·ª´ng load ƒë·ªÉ ƒë·∫∑t v√†o regions")
    print("- Bao g·ªìm visualization v·ªõi mapping lines v√† rotation commands")
    print("\nGhi ch√∫:")
    print("- T·∫•t c·∫£ c√°c demo ƒë·ªÅu s·ª≠ d·ª•ng Module Division ƒë·ªÉ chia pallet th√†nh c√°c v√πng nh·ªè")
    print("- Depth estimation ƒë∆∞·ª£c th·ª±c hi·ªán cho t·ª´ng v√πng ri√™ng bi·ªát")
    print("- Theta4 calculation ch·ªâ ho·∫°t ƒë·ªông khi c√≥ loads (class 0,1) v√† regions t·ª´ pallets (class 2)")
    print("- Sequential Region Sending (4) l√† t√≠nh nƒÉng ho√†n ch·ªânh cho robot g·∫Øp theo th·ª© t·ª±")
    print("- T·∫•t c·∫£ c√°c demo ƒë·ªÅu s·ª≠ d·ª•ng chung c·∫•u h√¨nh depth model")
    print("B·∫°n c√≥ th·ªÉ ƒë·∫∑t c√°c bi·∫øn m√¥i tr∆∞·ªùng ƒë·ªÉ ƒëi·ªÅu khi·ªÉn c√°c t√≠nh nƒÉng:")
    print("\nüè≠ PLC INTEGRATION:")
    print("  ENABLE_PLC: B·∫≠t/t·∫Øt PLC integration")
    print("    - ENABLE_PLC=true     # B·∫≠t PLC integration")
    print("    - ENABLE_PLC=false    # T·∫Øt PLC integration (m·∫∑c ƒë·ªãnh)")
    print("  PLC_IP: IP address c·ªßa PLC")
    print("    - PLC_IP=192.168.0.1  # IP PLC (m·∫∑c ƒë·ªãnh)")
    print("    - PLC_IP=192.168.1.100 # Example custom IP")
    print("\nüìè DEPTH MODEL:")
    print("  DEPTH_DEVICE: Thi·∫øt b·ªã ch·∫°y depth model")
    print("    - DEPTH_DEVICE=cuda   # Ch·∫°y tr√™n GPU (m·∫∑c ƒë·ªãnh n·∫øu c√≥ CUDA)")
    print("    - DEPTH_DEVICE=cpu    # Ch·∫°y tr√™n CPU")
    print("    - DEPTH_DEVICE=off    # T·∫Øt ho√†n to√†n depth model (m·∫∑c ƒë·ªãnh)")
    print("\n  DEPTH_TYPE: Lo·∫°i m√¥ h√¨nh depth")
    print("    - DEPTH_TYPE=regular  # Regular depth model (normalized output)")
    print("    - DEPTH_TYPE=metric   # Metric depth model (output in meters)")
    print("\n  DEPTH_MODEL: K√≠ch th∆∞·ªõc m√¥ h√¨nh ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô")
    print("    - DEPTH_MODEL=large   # M√¥ h√¨nh l·ªõn, ch·∫•t l∆∞·ª£ng cao, ch·∫≠m nh·∫•t")
    print("    - DEPTH_MODEL=base    # M√¥ h√¨nh v·ª´a, c√¢n b·∫±ng t·ªëc ƒë·ªô/ch·∫•t l∆∞·ª£ng")
    print("    - DEPTH_MODEL=small   # M√¥ h√¨nh nh·ªè, t·ªëc ƒë·ªô nhanh nh·∫•t (m·∫∑c ƒë·ªãnh)")
    print("\n  DEPTH_SCENE: Lo·∫°i c·∫£nh (ch·ªâ cho metric depth)")
    print("    - DEPTH_SCENE=indoor  # C·∫£nh trong nh√† (m·∫∑c ƒë·ªãnh)")
    print("    - DEPTH_SCENE=outdoor # C·∫£nh ngo√†i tr·ªùi")
    print("\n  DEPTH_SIZE: K√≠ch th∆∞·ªõc ƒë·∫ßu v√†o (W,H) ƒë·ªÉ tƒÉng t·ªëc")
    print("    - DEPTH_SIZE=640x480  # V√≠ d·ª•: 640x480")
    print("\n  DEPTH_SKIP: S·ªë frame b·ªè qua gi·ªØa c√°c l·∫ßn x·ª≠ l√Ω")
    print("    - DEPTH_SKIP=5        # V√≠ d·ª•: Ch·ªâ x·ª≠ l√Ω 1 frame trong m·ªói 6 frames")
    print("\n  SHOW_DEPTH: B·∫≠t/t·∫Øt hi·ªÉn th·ªã depth map")
    print("    - SHOW_DEPTH=true     # Hi·ªÉn th·ªã depth map (c√≥ th·ªÉ g√¢y lag)")
    print("    - SHOW_DEPTH=false    # T·∫Øt hi·ªÉn th·ªã depth map (m·∫∑c ƒë·ªãnh)")
    print("\n  SHOW_THETA4: B·∫≠t/t·∫Øt hi·ªÉn th·ªã theta4 calculation")
    print("    - SHOW_THETA4=true    # Hi·ªÉn th·ªã theta4 info (c√≥ th·ªÉ g√¢y lag)")
    print("    - SHOW_THETA4=false   # T·∫Øt hi·ªÉn th·ªã theta4 info (m·∫∑c ƒë·ªãnh)")
    print("\n  SHOW_REGIONS: B·∫≠t/t·∫Øt hi·ªÉn th·ªã region processing")
    print("    - SHOW_REGIONS=true   # Hi·ªÉn th·ªã regions v√† detections theo regions (m·∫∑c ƒë·ªãnh)")
    print("    - SHOW_REGIONS=false  # T·∫Øt hi·ªÉn th·ªã regions")
    print("\n  USE_CAMERA_CALIBRATION: B·∫≠t/t·∫Øt camera calibration")
    print("    - USE_CAMERA_CALIBRATION=true    # S·ª≠ d·ª•ng camera calibration (m·∫∑c ƒë·ªãnh)")
    print("    - USE_CAMERA_CALIBRATION=false   # T·∫Øt camera calibration")
    print("\n  CAMERA_CALIBRATION_FILE: ƒê∆∞·ªùng d·∫´n file camera calibration")
    print("    - CAMERA_CALIBRATION_FILE=camera_params.npz  # File calibration (m·∫∑c ƒë·ªãnh)")
    print("\n  V√≠ d·ª• v·ªõi Theta4 enabled:")
    print("    set SHOW_THETA4=true && python use_tensorrt_example.py")
    print("\n  V√≠ d·ª• Regular Depth v·ªõi Camera Calibration v√† Theta4:")
    print("    set DEPTH_DEVICE=cuda && set DEPTH_TYPE=regular && set DEPTH_MODEL=small && set USE_CAMERA_CALIBRATION=true && set SHOW_THETA4=true && python use_tensorrt_example.py")
    print("\n  V√≠ d·ª• Metric Depth (Indoor) v·ªõi Camera Calibration v√† Theta4:")
    print("    set DEPTH_DEVICE=cuda && set DEPTH_TYPE=metric && set DEPTH_SCENE=indoor && set DEPTH_MODEL=base && set USE_CAMERA_CALIBRATION=true && set SHOW_THETA4=true && python use_tensorrt_example.py")
    print("\n  V√≠ d·ª• Metric Depth (Outdoor) kh√¥ng c√≥ Camera Calibration:")
    print("    set DEPTH_DEVICE=cuda && set DEPTH_TYPE=metric && set DEPTH_SCENE=outdoor && set DEPTH_MODEL=small && set USE_CAMERA_CALIBRATION=false && python use_tensorrt_example.py")
    print("\n  V√≠ d·ª• v·ªõi file calibration t√πy ch·ªânh:")
    print("    set USE_CAMERA_CALIBRATION=true && set CAMERA_CALIBRATION_FILE=my_camera_calib.npz && python use_tensorrt_example.py")
    print("\n  V√≠ d·ª• v·ªõi Regions v√† Theta4 enabled:")
    print("    set SHOW_REGIONS=true && set SHOW_THETA4=true && python use_tensorrt_example.py")
    print("\n  V√≠ d·ª• full features (Depth + Theta4 + Regions):")
    print("    set DEPTH_DEVICE=cuda && set DEPTH_TYPE=metric && set SHOW_DEPTH=true && set SHOW_THETA4=true && set SHOW_REGIONS=true && python use_tensorrt_example.py")
    print()
    print("üè≠ PLC INTEGRATION EXAMPLES:")
    print("  V√≠ d·ª• v·ªõi PLC enabled (demo mode):")
    print("    set ENABLE_PLC=true && python use_tensorrt_example.py")
    print()
    print("  V√≠ d·ª• v·ªõi PLC th·∫≠t (custom IP):")
    print("    set ENABLE_PLC=true && set PLC_IP=192.168.1.100 && python use_tensorrt_example.py")
    print()
    print("  V√≠ d·ª• FULL FEATURES v·ªõi PLC:")
    print("    set ENABLE_PLC=true && set PLC_IP=192.168.0.1 && set DEPTH_DEVICE=cuda && set SHOW_THETA4=true && set SHOW_REGIONS=true && python use_tensorrt_example.py")
    print()
    print("üí° KHI PLC ENABLED:")
    print("  - Nh·∫•n 'n' ƒë·ªÉ TH·∫¨T S·ª∞ g·ª≠i regions v√†o PLC DB26")
    print("  - BAG PALLET TRACKING s·∫Ω ho·∫°t ƒë·ªông")
    print("  - Regions ƒë∆∞·ª£c map theo: loads‚ÜíDB26.0/4, pallets1‚ÜíDB26.12/16, pallets2‚ÜíDB26.24/28")
    print()
    
    choice = input("Ch·ªçn ch·∫ø ƒë·ªô (1/2/3/4): ")
    
    if choice == "1":
        demo_single_image()
    elif choice == "2":
        demo_camera()
    elif choice == "3":
        demo_batch_images()
    elif choice == "4":
        demo_sequential_region_sending()
    else:
        print("L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!") 