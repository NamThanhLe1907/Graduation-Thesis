import cv2
import numpy as np
import torch
from utility.camera_interface import CameraInterface
from utility.frame_processor import FrameProcessor
from utility.yolo_inference import YOLOInference
from utility.postprocessing import PostProcessor
from utility.performance_monitor import PerformanceMonitor
# from src.Module_division import DivisionModule
import time
from utility.depth_calculate_v2 import DepthEstimatorV2
import threading
from queue import Queue


class VideoProcessor:
    def __init__(self, gui):
        self.gui = gui
        self.camera = CameraInterface()
        self.processor = FrameProcessor()
        self.yolo_inference = YOLOInference(
            model_path="final.pt", 
            conf=0.9
        )
        # self.division_module = DivisionModule(pallet_config={'LR': 12, 'LQ': 10})
        self.post_processor = PostProcessor(alpha=0.2)
        self.performance_monitor = PerformanceMonitor()
        self.depth_estimator = DepthEstimatorV2(
            max_depth=1,
            encoder='vits',
            checkpoint_path='utility/checkpoint/depth_anything_v2_metric_hypersim_vits.pth',
            device='cuda'  # Th√™m device parameter
        )
        self.running = False

        # ƒê∆°n v·ªã hi·ªÉn th·ªã depth cho bounding box: 'cm' ho·∫∑c 'mm'
        self.depth_unit = 'cm'  # ho·∫∑c 'mm'

        # ƒê·ªãnh nghƒ©a b·∫£ng m√†u cho c√°c l·ªõp (b·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh theo s·ªë l∆∞·ª£ng v√† mong mu·ªën)
        self.color_map = {
            0: (255, 0, 0),   # Blue
            1: (0, 255, 0),   # Green
            2: (0, 0, 255),   # Red
            # Th√™m c√°c m√†u kh√°c n·∫øu c·∫ßn
        }
        # Bi·∫øn l∆∞u heatmap v√† depth map gi·∫£m t·∫£i inference
        self.last_depth_heatmap = None
        self.last_calibrated_depth = None
        self.frame_count = 0

    def start_processing(self):

        self.running = True
        self.camera.initialize()

        # Kh·ªüi t·∫°o lock v√† queue cho ƒë·ªìng b·ªô h√≥a
        self.frame_lock = threading.Lock()
        self.yolo_lock = threading.Lock()
        self.depth_lock = threading.Lock()
        self.frame_queue = Queue(maxsize=1)
        self.yolo_queue = Queue(maxsize=1)
        self.depth_queue = Queue(maxsize=1)

        # Thread YOLO (ch·∫°y tr∆∞·ªõc)
        def yolo_loop():
            while self.running:
                try:
                    # L·∫•y frame m·ªõi nh·∫•t t·ª´ queue
                    frame = self.frame_queue.get_nowait()
                    
                    # Ti·ªÅn x·ª≠ l√Ω v√† ch·∫°y YOLO
                    processed = self.processor.preprocess_frame(frame)
                    results = self.yolo_inference.infer(processed)
                    
                    with self.yolo_lock:
                        self.last_yolo_results = results
                        # Clear queue
                        while not self.yolo_queue.empty():
                            self.yolo_queue.get_nowait()
                        self.yolo_queue.put(results)
                        
                        # N·∫øu c√≥ k·∫øt qu·∫£ YOLO, ƒë·∫©y frame v√† results v√†o queue depth
                        if results and len(results) > 0 and results[0].obb:
                            while not self.depth_queue.empty():
                                self.depth_queue.get_nowait()
                            self.depth_queue.put((frame.copy(), results))
                    
                except:
                    pass
                time.sleep(0.001)

        # Thread Depth (ch·∫°y sau khi c√≥ YOLO)
        def depth_loop():
            frame_counter = 0
            while self.running:
                try:
                    # L·∫•y frame v√† k·∫øt qu·∫£ YOLO t·ª´ queue
                    try:
                        frame, results = self.depth_queue.get_nowait()
                    except Exception as e_queue:
                        import queue as py_queue
                        if isinstance(e_queue, py_queue.Empty):
                            # Queue r·ªóng, kh√¥ng ph·∫£i l·ªói, b·ªè qua
                            time.sleep(0.005)
                            continue
                        else:
                            # L·ªói kh√°c, in ra
                            import traceback
                            print(f"‚ùå [DEPTH ERROR] Frame #{frame_counter} | Queue error: {str(e_queue)}")
                            print(f"üìú [STACK TRACE]\n{traceback.format_exc()}")
                            time.sleep(0.01)
                            continue

                    frame_counter += 1
                    
                    if results and len(results) > 0 and results[0].obb:
                        obb = results[0].obb
                        print(f"üîç [DEPTH] Frame #{frame_counter} | Processing {len(obb)} objects")
                        start_time = time.time()
                        
                        # Ch·ªâ t√≠nh depth cho c√°c v√πng c√≥ object
                        for box in obb.xyxyxyxy:
                            # Crop frame v·ªõi boundary check
                            if isinstance(box, torch.Tensor):
                                pts = box.detach().cpu().numpy().reshape(4, 2).astype(int)
                            else:
                                pts = np.array(box).reshape(4, 2).astype(int)
                            x, y, w, h = cv2.boundingRect(pts)
                            # Validate boundary
                            x = max(0, x)
                            y = max(0, y)
                            w = min(w, frame.shape[1] - x)
                            h = min(h, frame.shape[0] - y)
                            if w <= 10 or h <= 10:  # Skip qu√° nh·ªè
                                print(f"‚ö†Ô∏è [SKIP] Box too small: {w}x{h}")
                                continue
                            cropped = frame[y:y+h, x:x+w]
                            
                            if cropped.size > 0 and len(cropped.shape) == 3:
                                # Chuy·ªÉn sang RGB v√† t√≠nh depth
                                rgb = cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)
                                if not rgb.flags['C_CONTIGUOUS']:
                                    rgb = np.ascontiguousarray(rgb)
                                
                                input_size = ((max(rgb.shape[:2]) + 13) // 14) * 14
                                depth = self.depth_estimator.model.infer_image(rgb, input_size=input_size)
                                # Dynamic scale based on max_depth
                                depth_scale = self.depth_estimator.max_depth / 10.0  # 10.0 l√† max_depth m·∫∑c ƒë·ªãnh c·ªßa model
                                depth = depth * depth_scale
                                
                                # Debug v√† x·ª≠ l√Ω depth map
                                print(f"üîß [DEBUG] Depth type: {type(depth)}, shape: {depth.shape if hasattr(depth, 'shape') else 'N/A'}")
                                try:
                                    if isinstance(depth, torch.Tensor):
                                        print(f"‚öôÔ∏è [TENSOR INFO] Device: {depth.device}, Type: {depth.dtype}")
                                        depth_np = depth.detach().cpu().numpy()
                                        print(f"üîÑ [CONVERSION] Converted to numpy array | Shape: {depth_np.shape}")
                                    elif isinstance(depth, np.ndarray):
                                        depth_np = depth
                                        print(f"üî¢ [NUMPY ARRAY] Direct use | Shape: {depth_np.shape}")
                                    else:
                                        # Tr∆∞·ªùng h·ª£p kh√¥ng r√µ ki·ªÉu d·ªØ li·ªáu
                                        print(f"‚ö†Ô∏è [WARNING] Depth data is neither Tensor nor ndarray. Type: {type(depth)}. Attempting to convert to numpy array.")
                                        try:
                                            depth_np = np.array(depth)
                                            print(f"‚úÖ [CONVERTED] Converted to numpy array | Shape: {depth_np.shape}")
                                        except Exception as e_conv:
                                            print(f"‚ùå [ERROR] Failed to convert depth to numpy array: {e_conv}")
                                            continue  # b·ªè qua frame n√†y
                                    
                                    print(f"üìä [DEPTH STATS] Min: {depth_np.min():.2f}, Max: {depth_np.max():.2f}")
                                    
                                    with self.depth_lock:
                                        # ƒê·∫£m b·∫£o depth_np l√† numpy array tr∆∞·ªõc khi g·ªçi astype
                                        if not isinstance(depth_np, np.ndarray):
                                            try:
                                                depth_np = np.array(depth_np)
                                            except:
                                                print(f"‚ùå [ERROR] Cannot convert depth to numpy array for astype(). Skipping this frame.")
                                                continue
                                        self.last_calibrated_depth = depth_np.astype(np.float32)
                                        print(f"üîí [LOCK ACQUIRED] Depth map saved")
                                        
                                        heatmap, _ = self.depth_estimator.get_heatmap(depth_np, unit='cm')
                                        print(f"üé® [HEATMAP] Generated | Shape: {heatmap.shape}")
                                        self.last_depth_heatmap = heatmap
                                    
                                except Exception as e:
                                    import traceback
                                    print(f"üî• [DEPTH ERROR] Frame #{frame_counter} | {str(e)}")
                                    print(f"üìú [STACK TRACE]\n{traceback.format_exc()}")
                                    torch.cuda.empty_cache()
                                    time.sleep(0.1)  # Gi·∫£m t·∫£i GPU
                                continue
                                
                            torch.cuda.empty_cache()
                        
                        proc_time = time.time() - start_time
                        print(f"‚úÖ [DEPTH] Frame #{frame_counter} | Processed in {proc_time:.2f}s")
                                
                except Exception as e:
                    import traceback
                    print(f"‚ùå [DEPTH ERROR] Frame #{frame_counter} | {str(e)}")
                    print(f"üìú [STACK TRACE]\n{traceback.format_exc()}")
                time.sleep(0.01)

        threading.Thread(target=yolo_loop, daemon=True).start()
        threading.Thread(target=depth_loop, daemon=True).start()

        while self.running:
            try:
                self.performance_monitor.start_frame()
                self.frame_count += 1

                # L·∫•y khung h√¨nh t·ª´ camera
                frame = self.camera.get_frame()
                if frame is None:
                    continue

                # T√çNH FULL FRAME DEPTH NGAY SAU KHI L·∫§Y FRAME
                try:
                    rgb_full = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    if not rgb_full.flags['C_CONTIGUOUS']:
                        rgb_full = np.ascontiguousarray(rgb_full)
                    # ƒê·∫£m b·∫£o k√≠ch th∆∞·ªõc chia h·∫øt cho 14
                    input_size = ((max(rgb_full.shape[:2]) + 13) // 14) * 14
                    depth_full = self.depth_estimator.model.infer_image(rgb_full, input_size=input_size)
                    # Scale depth theo max_depth
                    scale_factor = self.depth_estimator.max_depth / 10.0
                    depth_full = depth_full * scale_factor

                    # Convert tensor sang numpy n·∫øu c·∫ßn
                    if isinstance(depth_full, torch.Tensor):
                        depth_full_np = depth_full.detach().cpu().numpy()
                    elif isinstance(depth_full, np.ndarray):
                        depth_full_np = depth_full
                    else:
                        depth_full_np = np.array(depth_full)

                    self.last_calibrated_depth = depth_full_np.astype(np.float32)
                except:
                    pass
# T·∫°o heatmap cho full frame depth heatmap_full, _ = self.depth_estimator.get_heatmap(depth_full_np, unit='cm') self.last_depth_heatmap = heatmap_full except Exception as e: print(f"[FULL FRAME DEPTH ERROR] {e}")

                # ƒê·∫©y frame v√†o queue v√† ƒë·∫£m b·∫£o ch·ªâ gi·ªØ frame m·ªõi nh·∫•t
                try:
                    # Clear queue n·∫øu c√≥ nhi·ªÅu h∆°n 1 frame ch·ªù x·ª≠ l√Ω
                    while not self.frame_queue.empty():
                        self.frame_queue.get_nowait()
                    self.frame_queue.put_nowait(frame.copy())
                except:
                    pass

                # Ti·ªÅn x·ª≠ l√Ω khung h√¨nh
                processed_frame = self.processor.preprocess_frame(frame)
                annotated_frame = processed_frame.copy()
                img_w, img_h = frame.shape[1], frame.shape[0]
                img_size = (img_w, img_h)
                
                # L·∫•y k·∫øt qu·∫£ YOLO m·ªõi nh·∫•t
                results = getattr(self, 'last_yolo_results', None)
                  
                # X·ª≠ l√Ω k·∫øt qu·∫£ YOLO (nh∆∞ tr∆∞·ªõc)
                if results and len(results) > 0:
                    if results[0].obb:
                        obb = results[0].obb
                        confs = obb.conf.detach().cpu().numpy()
                        print(f"Confidence stats - Min: {confs.min():.2f}, Max: {confs.max():.2f}, Mean: {confs.mean():.2f}")
                        valid_conf_indices = np.where(confs > 0.9)[0]
                        if valid_conf_indices.size == 0:
                            continue
                        confs = confs[valid_conf_indices]
                        boxes_all = obb.xywhr.detach().cpu().numpy()
                        boxes = boxes_all[valid_conf_indices]
                        original_count = len(boxes)
                        filtered_result = self.post_processor.filter_by_geometry(
                            boxes,
                            frame_resolution=img_size
                        )
                        if isinstance(filtered_result, tuple):
                            boxes, valid_indices = filtered_result
                            confs = confs[valid_indices]
                        else:
                            boxes = filtered_result
                            valid_indices = None
                        print(f"Geometry filter: Kept {len(boxes)}/{original_count} boxes")
                            
                        collisions = self.post_processor.detect_collisions(
                            obb,
                            frame_resolution=img_size,
                            confs=confs,
                            valid_indices=valid_indices
                        )
                        
                        annotated_frame = processed_frame.copy()
                        rotated_boxes = obb.xyxyxyxy.detach().cpu().numpy()
                        indices = valid_indices if valid_indices is not None else range(len(rotated_boxes))
                        
                        from collections import defaultdict
                        class_depths = defaultdict(list)

                        for idx in indices:
                            poly = rotated_boxes[idx]
                            cls_id = obb.cls.detach().cpu().numpy()[idx]
                            conf = confs[idx]
                            color = self.color_map.get(cls_id, (255, 255, 0))
                            
                            if hasattr(results[0], "names"):
                                names = results[0].names
                            else:
                                names = {0: "-load-", 1: "-pallet-"}
                            
                            label = names.get(cls_id, f"ID:{cls_id}")
                            angle_deg = float(np.rad2deg(obb.xywhr[idx, 4])) if idx < len(obb.xywhr) else None
                            
                            label_text = f"{label} {conf:.2f}" + (f", {angle_deg:.2f}deg" if angle_deg else "")
                            
                            pts = poly.reshape(4, 2).astype(int)
                            cv2.polylines(annotated_frame, [pts], isClosed=True, color=color, thickness=4)
                            
                            # T√≠nh to√°n v√† hi·ªÉn th·ªã depth cho t·ª´ng box
                            if self.last_calibrated_depth is not None:
                                try:
                                    x, y, w, h = cv2.boundingRect(pts)
                                    # Gi·ªõi h·∫°n v√πng kh√¥ng v∆∞·ª£t qu√° k√≠ch th∆∞·ªõc depth map
                                    x_end = min(x + w, self.last_calibrated_depth.shape[1])
                                    y_end = min(y + h, self.last_calibrated_depth.shape[0])
                                    region = self.last_calibrated_depth[y:y_end, x:x_end]
                                    if region.size > 0:
                                        if self.depth_unit == 'mm':
                                            scale_factor = 1000
                                            unit_label = 'mm'
                                            decimals = 0
                                        else:  # m·∫∑c ƒë·ªãnh cm
                                            scale_factor = 100
                                            unit_label = 'cm'
                                            decimals = 3
                                        d_min = region.min() * scale_factor
                                        d_mean = region.mean() * scale_factor
                                        d_max = region.max() * scale_factor
                                        label_text += f" | D: {d_mean:.{decimals}f}{unit_label}"
                                        # L∆∞u mean depth v√†o dict theo class
                                        class_depths[cls_id].append(d_mean)
                                except Exception as e:
                                    pass
                            
                            xmin = pts[:,0].min()
                            ymax = pts[:,1].max()
                            text_pos = (int(xmin), int(ymax))
                            cv2.putText(annotated_frame, label_text, text_pos,
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 2)
                        
                        # Sau khi duy·ªát h·∫øt c√°c box, t√≠nh trung b√¨nh theo class
                        # Gh√©p th√¥ng tin kho·∫£ng c√°ch trung b√¨nh theo class
                        class_depth_info = ""
                        for cls_id, depth_list in class_depths.items():
                            if len(depth_list) > 0:
                                mean_depth_class = np.mean(depth_list)
                                line = f"[Class {cls_id}] Mean distance: {mean_depth_class:.2f} {unit_label} ({len(depth_list)} boxes)"
                                print(line)
                                class_depth_info += line + "\n"

                        # T√≠nh kho·∫£ng c√°ch trung b√¨nh to√†n ·∫£nh (full frame)
                        full_frame_info = ""
                        if self.last_calibrated_depth is not None:
                            try:
                                full_mean = np.mean(self.last_calibrated_depth) * (1000 if self.depth_unit=='mm' else 100)
                                full_min = np.min(self.last_calibrated_depth) * (1000 if self.depth_unit=='mm' else 100)
                                full_max = np.max(self.last_calibrated_depth) * (1000 if self.depth_unit=='mm' else 100)
                                full_frame_info = f"[Full Frame] Mean: {full_mean:.2f} {unit_label}, Min: {full_min:.2f} {unit_label}, Max: {full_max:.2f} {unit_label}"
                                print(full_frame_info)
                            except:
                                pass
                        
                        # N·∫øu c√≥ th√¥ng tin full frame, overlay l√™n annotated_frame
                        if full_frame_info:
                            try:
                                cv2.putText(annotated_frame, full_frame_info, (10, 20),
                                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                            except Exception as e:
                                print(f"Error overlaying full frame info: {e}")

                        # Hi·ªÉn th·ªã l√™n GUI console
                        combined_info = ""
                        if full_frame_info:
                            combined_info += full_frame_info + "\n"
                        if class_depth_info.strip():
                            combined_info += class_depth_info.strip()
                        if combined_info.strip():
                            self.gui.log_message(combined_info.strip(), level="INFO")
                        
                        for (i, j) in collisions:
                            box1 = boxes[i]
                            box2 = boxes[j]
                            x_center1 = int(box1[0])
                            y_center1 = int(box1[1])
                            center1 = (x_center1, y_center1)
                            
                            x_center2 = int(box2[0])
                            y_center2 = int(box2[1])
                            center2 = (x_center2, y_center2)
                            cv2.circle(annotated_frame, center1, 5, (0, 0, 255), -1)
                            cv2.circle(annotated_frame, center2, 5, (0, 0, 255), -1)
                            cv2.line(annotated_frame, center1, center2, (0, 0, 255), 2)
                            
                        diff = cv2.norm(annotated_frame, processed_frame)
                        self.gui.log_message(f"Annotation applied with postprocessing, diff={diff}, shape={annotated_frame.shape}", "DEBUG")
                else:
                    print("No inference results")
                
                # N·∫øu ƒë√£ c√≥ heatmap t·ª´ v√≤ng tr∆∞·ªõc, s·ª≠ d·ª•ng l·∫°i
                # ƒê·∫£m b·∫£o annotated_frame l√† numpy array h·ª£p l·ªá
                if annotated_frame is not None and isinstance(annotated_frame, np.ndarray):
                    if self.last_depth_heatmap is not None and isinstance(self.last_depth_heatmap, np.ndarray):
                        if annotated_frame.shape == self.last_depth_heatmap.shape:
                            try:
                                combined_frame = cv2.addWeighted(annotated_frame, 0.7, self.last_depth_heatmap, 0.3, 0)
                            except Exception as e:
                                print(f"Error combining frames: {str(e)}")
                                combined_frame = annotated_frame.copy()
                        else:
                            combined_frame = annotated_frame.copy()
                    else:
                        combined_frame = annotated_frame.copy()
                    
                    # L∆∞u file debug n·∫øu c·∫ßn
                    try:
                        cv2.imwrite("debug_output.jpg", combined_frame)
                    except Exception as e:
                        print(f"Error saving debug image: {str(e)}")
                else:
                    combined_frame = np.zeros((480, 640, 3), dtype=np.uint8)

                if combined_frame is not None and len(combined_frame.shape) == 3:
                    if combined_frame.shape[2] == 4:
                        display_frame = cv2.cvtColor(combined_frame, cv2.COLOR_BGRA2BGR)
                    else:
                        display_frame = combined_frame.copy()
                else:
                    display_frame = np.zeros((480, 640, 3), dtype=np.uint8)
                self.gui.root.after(0, lambda: self.gui.update(display_frame))
                if self.last_depth_heatmap is not None:
                    self.gui.root.after(0, lambda: self.gui.update_depth(self.last_depth_heatmap))

                self.performance_monitor.end_frame()
                fps = self.performance_monitor.get_fps()
                avg_inference = self.performance_monitor.get_average_inference_time()
                self.gui.update_fps_info(fps, avg_inference)
                time.sleep(0.01)

            except Exception as e:
                print(f"Processing error: {str(e)}")
                self.gui.log_message(f"Processing error: {str(e)}", "ERROR")
                break

        self.camera.release()
        self.gui.log_message("Camera released", "INFO")
        print("Camera released")
